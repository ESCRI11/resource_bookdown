[
["index.html", "Orchestrating non-disclosive big data analyses of shared data from different resources with R and DataSHIELD Wellcome", " Orchestrating non-disclosive big data analyses of shared data from different resources with R and DataSHIELD 2020-05-08 Wellcome This is the website for a book that provides users with some common workflows for the non-disclosive analysis of biomedical data with R and DataSHIELD from different resources. This book will teach you how to use the resourcer R package to perform any statistcal analysis from different studies having data in different formats (e.g., CSV, SPSS, R class, …). In particular, we focus on illustrating how to deal with Big Data by providing several examples from omic and geographical settings. To this end, we use cutting-edge Bioconductor tools to perform transcriptomic, epigenomic and genomic data analyses. Serveral R packages are used to perform analysis of geospatial data. We also provide examples of how performing non-disclosive analyses using secure SHELL commands by allowing the use of specific software that properly deals with Big Data outside R. This material serves as an online companion for the manuscript “Orchestrating non-disclosive big data analyses of shared data from different resources with R and DataSHIELD”. While we focus here in genomic and geoespatial data, dozens of data analysis aplications interested in performing non-disclosive analysis having data in any specific format could be carried out. By learning the grammar of DataSHILED workflows, we hope to provide you a starting point for the exploration of your own data, whether it be omic, geospatial or otherwise. This book is organized into five parts. In the Preamble, we introduce the book, provides a tutorial for key data infrastructure useful for omic and geospatial data and a general overview for learning how Opal and DataSHILED allows performing non-disclosive data analyses from multiple studies simultaneously. So far, DataSHIELD uses tables from repository data in Opal which have some limitations to perfom Big Data analyses. The second part, Focus Topic, dive into information for for non-disclosive analyses using any type of resource which is one of the key advances provided in this work. It allow, among others, perform big data analyses using genomic or geospatial information where thousand of sensitive data have to be managed. The third part, Resources Extensions, provides examples illustrating how to extend existing resources. We describe how to create functions to deal with omic data in Variant Calling Format (VCF files) which specifies the format of a text file used in bioinformatics for storing gene sequence variations. It also shows how to perform genomic data analysis using secure shell commands. This can be consider as an illustrative example for using shell programs to perform Big Data analyses that could be extend to other frameworks of Big Data such as “Apache Spark”. The fourth part, Workflows, provides primarily code detailing the analysis of various datasets throughout the book. Initially, we provide examples from transcriptomic, epigenomic and genomic association studies, as well as examples linking geospatial data where data confidenciallity is an important issue. The fifth part, Developers, provides information about how to develop DataSHIELD packages using specific R packages devoted to Big Data analyses as well as using shell programs. It also provides some useful trips and trick that developers or end users may face when using our proposed infrastructure. Finally, the Appendix highlights our contributors. If you would like to cite this work, please use the reference “Orchestrating non-disclosive big data analyses of shared data from different resources with R and DataSHIELD”. "],
["introduction.html", "1 Introduction", " 1 Introduction "],
["r.html", "2 R", " 2 R "],
["BioC.html", "3 Bioconductor data infrastructures", " 3 Bioconductor data infrastructures "],
["opal.html", "4 OPAL", " 4 OPAL Opal is OBiBa’s core database application for biobanks. Participant data, once collected either from OBiBa’s Onyx application, must be integrated and stored in a central data repository under a uniform model. Opal is such a central repository. Current Opal version can import, process, and copy data. Opal is typically used in a research center to analyze the data acquired at assessment centres. Its ultimate purpose is to achieve seamless data-sharing among biobanks. More information on Opal future can be seen in Opal description on OBiBa. Initial implementation of Opal was based on managing databases as tables by supporting several data warehouse technologies such as: MongoDB , Mysql , MariaDB and PostgreSQL as database software backend, Import data from CSV, SPSS, SAS, Stata files and from SQL databases, Export data to CSV, SPSS, SAS, Stata files and to SQL databases, Connect directly to multiple data source software such as SQL databases and LimeSurvey , Store data about any type of “entity”, such as subject, sample, geographic area, etc., Store data of any type (e.g., texts, numbers, geo-localisation, images, videos, etc.), Import and store genotype data as VCF files (Variant Call format ), … Detailed concepts and tutorials for tables can be found here: Variables and Data Identifiers Mappings Data Harmonization To have a closer look at Opal with table you can try our demo site: username: administrator password: password In this work, we will present a more flexible data infrastructure called resources that will be able to manage Big Data sets which can be a limitation when using tables. Before introducing this new feature and in order to make the reader familiar with Opal we recommend to visit this webpage where a global overview along with some examples are described "],
["DataSHIELD.html", "5 DataSHIELD 5.1 R and DataSHIELD implementation in Opal 5.2 R module 5.3 DataSHIELD module 5.4 Web Services 5.5 R Server Component 5.6 R Clients (Analysis Computers)", " 5 DataSHIELD DataSHIELD infrastructure is a software solution that allows simultaneous co-analysis of data from multiple studies stored on different servers without the need to physically pool data or disclose sensitive information. DataSHIELD uses Opal servers to properly perform such analyses. At a high level DataSHIELD is set up as a client-server model which houses the data for a particular study. A request is made from the client to run specific functions on the remote servers where the analysis is performed. Non-sensitive and pre-approved summary statistics are returned from each study to the client where they can be combined for an overall analysis. An overview of what a single-site DataSHIELD architecture would look like is illustrated in Figure 5.1. Figure 5.1: Single Server DataSHIELD Architecture (Wilson et al 2017) 5.1 R and DataSHIELD implementation in Opal Opal uses the R statistical environment to implement DataSHIELD. The implementation is made of 3 components: an Opal server an R server (using Rserve) an R package for Opal (installed on the Analysis Computer) Figure 5.2: R and DataSHIELD implementation in Opal 5.2 R module Used for the interaction between an R statistical environment and Opal. Specifically, this module allows pushing data from Opal into an R environment and back. It can also execute arbitrary R code within these environments. Opal interacts with an R server through Rserve’s protocol. This allows the R Server to be on a different machine than the Opal server. It also allows maintaining R separately from Opal. 5.3 DataSHIELD module Built “on top” of the R module, this provides a constrained and customisable access to the R environment. Specifically, this module allows pushing data from Opal into R, but does not allow reading this data unless it has first been aggregated which is a non-disclosive way of obtained data from repositories. The term “aggregated” here means that the data in R must go through a method that will summarize individual-level data into another form that removes the original individual-level data. For example, obtaining the length of a vector, obtaining the summary statistics of a vector (min, max, mean, etc.) It is these methods that are customisable. That is, administrators of the Opal server can add, remove, modify and create completely custom aggregating methods that are provided to DataSHIELD clients. 5.4 Web Services Allows the interaction between these modules and their clients is done through Web Services. 5.5 R Server Component R is made accessible to Opal through the Rserve library. This allows running R commands from several remote clients. Doing so allows running R and Opal on different machines if necessary. Note that this R Server will eventually contain individual-level data (it will be pushed there by the Opal server). This R server should be secured just like other machines involved in handling individual-level data. This data is not made directly available to Opal clients. 5.6 R Clients (Analysis Computers) The interaction between the analysis computer and Opal is done through another R environment running on the analysis computer. To support these interactions, Opal provides an R package that can be installed using normal R functionalities (CRAN). Clients can then use this package to authenticate to Opal instances and interact with the DataSHIELD methods offered by these servers. Readers can read this link to have a global overview about how to use DataSHIELD functions. It describes how to perform basic statistical analyses, lineal and generalized lineal models and some data visualization. A complete description of how DataSHIELD works, with lots of materials, examples, courses and real data analyses can be obtaine in the DataSHIELD Wiki. Just for a simple illustration, here we illustrate how to analyze some data available in our demo site. The tab Explore Data made access to the different projects that are avaialble in our Opal server. If we select “SURVIVAL” prject we see that there are three tables: Figure 5.3: Tables available in the SURVIVAL project from our Opal example R code to be provided …. "],
["dsi-datashield-interface-implementation-for-opal-data-repository.html", "6 DSI: DataSHIELD Interface implementation for Opal data repository 6.1 Class Structure 6.2 Higher Level Functions 6.3 Options", " 6 DSI: DataSHIELD Interface implementation for Opal data repository The DataSHIELD Interface (DSI) defines a set of S4 classes and generic methods that can be implemented for accessing a data repository supporting the DataSHIELD infrastructure: controlled R commands to be executed on the server side are garanteeing that non disclosive information is returned to client side. 6.1 Class Structure The DSI classes are: DSObject a common base class for all DSI, DSDriver drives the creation of a connection object, DSConnection allows the interaction with the remote server; DataSHIELD operations such as aggregation and assignment return a result object; DataSHIELD setup status check can be performed (dataset access, configuration comparision), DSResult wraps access to the result, which can be fetched either synchronously or asynchronously depending on the capabilities of the data repository server. All classes are virtual: they cannot be instantiated directly and instead must be subclassed. See DSOpal for a reference implementation of DSI based on the Opal data repository. These S4 classes and generic methods are meant to be used for implementing connection to a DataSHIELD-aware data repository. 6.2 Higher Level Functions In addition to these S4 classes, DSI provides functions to handle a list of remote data repository servers: datashield.login and datashield.logout will make use of the DSDriver paradigm to create DSConnections to the data repositories, datashield.aggregate and datashield.assign will perform typical DataSHIELD operations on DSConnections, which result will be fetched through DSResult objects, datashield.connections, datashield.connections_default and datashield.connections_find are functions for managing the list of DSConnection objects that will be discovered and used by the client-side analytic functions. Other data management functions are provided by the DSConnection objects: datashield.workspaces, datashield.workspace_save and datashield.workspace_rm allow to manage R images of the remote DataSHIELD sessions (to speed up data analysis sessions), datashield.symbols and datashield.symbol_rm offer a minimalistic management of the R symbols living in the remote DataSHIELD sessions, datashield.table_status, datashield.pkg_status, datashield.method_status and datashield.methods are utility functions to explore the DataSHIELD setup across a set of data repositories, These datashield.* functions are meant to be used by DataSHIELD packages developers and users. 6.3 Options Some options can be set to modify the behavior of the DSI: datashield.env is the R environment in which the DSConnection object list is to be looking for. Default value is the Global Environment: globalenv(). datashield.progress is a logical to enable the visibility of the progress bars. Default value is TRUE. datashield.progress.clear is a logical to make the progress bar disappear after it has been completed. Default value is FALSE. "],
["resources.html", "7 The resources 7.1 Concept 7.2 Types of resources", " 7 The resources Developing and implementing new algorithms to perform advanced data analyses under DataSHIELD framework is a current active line of research. However, the analysis of big data within DataSHIELD has some limitations. Some of them are related to how data is managed in Opal and others are related to how to perform statistical analyses of big data within the R environment. Opal databases do not properly manage large amounts of information and, second, it requires moving data from original repositories that is inefficient. We have overcome the problem related to DataSHIELD big data management by developing a new data infrastructure within Opal: the resources. 7.1 Concept Resources are datasets or computation units which location is described by a URL and access is protected by credentials. When assigned to a R/DataSHIELD server session, remote big/complex datasets or high performance computers are made accessible to data analysts. Instead of storing the data in Opal’s database, only the way to access them is to be defined: the datasets are kept in their original format and location (a SQL database, a SPSS file, R object, etc.) and are read directly from the R/DataSHIELD server-side session. Then as soon as there is a R reader for the dataset or a connector for the analysis services, a resource can be defined. Opal takes care of the DataSHIELD permissions (a DS user cannot see the resource’s credentials) and of the resources assignment to a R/DataSHIELD session (see Figure 7.1) Figure 7.1: Resources: a new DataSHIELD infrastructure 7.2 Types of resources The data format refers to the intrinsic structure of the data. A very common family of data formats is the tabular format which is made of rows (entities, records, observations etc.) and columns (variables, fields, vectors etc.). Examples of tabular formats are the delimiter-separated values formats (CSV, TSV etc.), the spreadsheet data formats (Microsoft Excel, LibreOffice Calc, Google Sheets etc.), some proprietary statistical software data formats (SPSS, SAS, Stata etc.), the database tables that can be stored in structured database management systems that are row-oriented (MySQL, MariaDB, PostgreSQL, Oracle, SQLite etc.) or column-oriented (Apache Cassandra, Apache Parquet, MariaDB ColumnStore, BigTable etc.), or in semi-structured database management systems such as the documented-oriented databases (MongoDB, Redis, CouchDB, Elasticsearch etc.). When the data model is getting complex (data types and objects relationships), a domain-specific data format is sometimes designed to handle this complexity so that statistical analysis and data retrieval can be executed as efficiently as possible. Examples of domain-specific data formats are encountered in the omic or geospatial fields of research that are described in the Workflows part: Omic and Geospatial. A data format can also include some additional features such as data compression, encoding or encryption. Each data format requires an appropriate reader software library or application to extract the information or perform data aggregation or filtering operations. We have prepared a test environment, with the Opal implementation of Resources and an appropriate R/DataSHIELD configuration that is available at: opal-test.obiba.org. This figure illustrate the resources which are available for the test project and can serve as a starting example of the different types of resources that can be dealt with Figure 7.2: Resources from a test enviroment available at https://opal-test.obiba.org As it can be seen, the data storage can simply be a file to be accessed directly from the host’s file system or to be downloaded from a remote location. More advanced data storage systems are software applications that expose an interface to query, extract or analyse the data. These applications can make use of a standard programming interface (e.g. SQL) or expose specific web services (e.g. based on the HTTP communication protocol) or provide a software library (in different programming languages) to access the data. These different ways of accessing the data are not exclusive from each other. In some cases the micro-data cannot be extracted, only computation services that return aggregated data are provided. The data storage system can also apply security rules, requiring authentication and proper authorisations to access or analyse the data. We call resource this data or computation access description. A resource will have the following properties: (1) the location of the data or of the computation services, (2) the data format (if this information cannot be inferred from the location property), (3) the access credentials (if some apply). The resource location description will make use of the web standard described in the RFC 3986 “Uniform Resource Identifier (URI): Generic Syntax”. More specifically, the Uniform Resource Locator (URL) specification is what we need for defining the location of the data or computation resource: the term Uniform allows to describe the resource the same way, independently of its type, location and usage context; the term Resource does not limit the scope of what might be a resource, e.g. a document, a service, a collection of resources, or even abstract concepts (operations, relationships, etc.); the term Locator both identifies the resource and provides a means of locating it by describing its access mechanism (e.g. the network location). The URL syntax is composed of several parts: (1) a scheme, that describes how to access the resource, e.g. the communication protocols “https” (secured HTTP communication), “ssh” (secured shell, for issuing commands on a remote server), or “s3” (for accessing Amazon Web Service S3 file store services), (2) an authority (optional), e.g. a server name address, (3) a path that identifies/locates the resource in a hierarchical way and that can be altered by query parameters. The resource’s data format might be inferred from the path part of the URL, by using the file name suffix for instance. Nevertheless, sometimes it is not possible to identify the data format because the path could make sense only for the data storage system, for example when a file store designates a document using an obfuscated string identifier or when a text-based data format is compressed as a zip archive. The format property can provide this information. Despite the authority part of the URL can contain some user information (such as the username and password), it is discouraged to use this capability for security considerations. The resource’s credentials property will be used instead, and will be composed of an identifier sub-property and a secret sub-property, which can be used for authenticating with a username/password, or an access token, or any other credentials encoded string. The advantage of separating the credentials property from the resource location property is that a user with limited permissions could have access to the resource’s location information while the credentials are kept secret. Once a resource has been formally defined, it should be possible to build programmatically a connection object that will make use of the described data or computation services. This resource description is not bound to a specific programmatic language (the URL property is a web standard, other properties are simple strings) and does not enforce the use of a specific software application for building, storing and interpreting a resource object. Next Section describes the resourcer package which is an R implementation of the data and computation resources description and connection. There the reader can see some examples of how dealing with different resources in DataSHIELD. "],
["resourcer.html", "8 The resourcer package 8.1 File Resources 8.2 File Getter 8.3 File Data Format 8.4 Database Resources 8.5 Computation Resources 8.6 Extending Resources 8.7 API declaration of a new resource 8.8 Resource Forms 8.9 Examples with different resources", " 8 The resourcer package The resourcer package is an R implementation of the data and computation resources description and connection. It is reusing many existing R packages for reading various data formats and connecting to external data storage or computation servers. The resourcer package role is to interpret a resource description object to build the appropriate resource connection object. Because the bestiary of resources is very wide, the resourcer package provides a framework for dynamically extending the interpretation capabilities to new types of resources. This framework uses the object-oriented paradigm provided by the R6 library. It is meant to access resources identified by a URL in a uniform way whether it references a dataset (stored in a file, a SQL table, a MongoDB collection etc.) or a computation unit (system commands, web services etc.). Usually some credentials will be defined, and an additional data format information can be provided to help dataset coercing to a data.frame object. The main concepts are: Resource, access to a resource (dataset or computation unit) is described by an object with URL, optional credentials and optional data format properties, ResourceResolver, a ResourceClient factory based on the URL scheme and available in a resolvers registry, ResourceClient, realizes the connection with the dataset or the computation unit described by a Resource, FileResourceGetter, connect to a file described by a resource, DBIResourceConnector, establish a DBI connection. 8.1 File Resources These are resources describing a file. If the file is in a remote location, it must be downloaded before being read. The data format specification of the resource helps to find the appropriate file reader. 8.2 File Getter The file locations supported by default are: file, local file system, http(s), web address, basic authentication, gridfs, MongoDB file store, scp, file copy through SSH, opal, Opal file store. This can be easily applied to other file locations by extending the FileResourceGetter class. An instance of the new file resource getter is to be registered so that the FileResourceResolver can operate as expected. resourcer::registerFileResourceGetter(MyFileLocationResourceGetter()$new()) 8.3 File Data Format The data format specified within the Resource object, helps at finding the appropriate file reader. Currently supported data formats are: the data formats that have a reader in tidyverse: readr (csv, csv2, tsv, ssv, delim), haven (spss, sav, por, dta, stata, sas, xpt), readxl (excel, xls, xlsx). This can be easily applied to other data file formats by extending the FileResourceClient class. the R data format that can be loaded in a child R environment from which object of interest will be retrieved. Usage example that reads a local SPSS file: # make a SPSS file resource res &lt;- resourcer::newResource( name = &quot;CNSIM1&quot;, url = &quot;file:///data/CNSIM1.sav&quot;, format = &quot;spss&quot; ) # coerce the csv file in the opal server to a data.frame df &lt;- as.data.frame(res) To support other file data format, extend the FileResourceClient class with the new data format reader implementation. Associate factory class, an extension of the ResourceResolver class is also to be implemented and registered. resourcer::registerResourceResolver(MyFileFormatResourceResolver$new()) 8.4 Database Resources 8.4.1 DBI Connectors DBI is a set of virtual classes that are are used to abstract the SQL database connections and operations within R. Then any DBI implementation can be used to access to a SQL table. Which DBI connector to be used is an information that can be extracted from the scheme part of the resource’s URL. For instance a resource URL starting with postgres:// will require the RPostgres driver. To separate the DBI connector instanciation from the DBI interface interactions in the SQLResourceClient, a DBIResourceConnector registry is to be populated. The currently supported SQL database connectors are: mariadb MariaDB connector, mysql MySQL connector, postgres or postgresql Postgres connector, presto, presto+http or presto+https Presto connector, spark, spark+http or spark+https Spark connector. To support another SQL database having a DBI driver, extend the DBIResourceConnector class and register it: resourcer::registerDBIResourceConnector(MyDBResourceConnector$new()) 8.4.2 Use dplyr Having the data stored in the database allows to handle large (common SQL databases) to big (PrestoDB, Spark) datasets using dplyr which will delegate as much as possible operations to the database. 8.4.3 Document Databases NoSQL databases can be described by a resource. The nodbi can be used here. Currently only connection to MongoDB database is supported using URL scheme mongodb or mongodb+srv. 8.5 Computation Resources Computation resources are resources on which tasks/commands can be triggerred and from which resulting data can be retrieved. Example of computation resource that connects to a server through SSH: # make an application resource on a ssh server res &lt;- resourcer::newResource( name = &quot;supercomp1&quot;, url = &quot;ssh://server1.example.org/work/dir?exec=plink,ls&quot;, identity = &quot;sshaccountid&quot;, secret = &quot;sshaccountpwd&quot; ) # get ssh client from resource object client &lt;- resourcer::newResourceClient(res) # does a ssh::ssh_connect() # execute commands files &lt;- client$exec(&quot;ls&quot;) # exec &#39;cd /work/dir &amp;&amp; ls&#39; # release connection client$close() # does ssh::ssh_disconnect(session) 8.6 Extending Resources There are several ways to extend the Resources handling. These are based on different R6 classes having a isFor(resource) function: If the resource is a file located at a place not already handled, write a new FileResourceGetter subclass and register an instance of it with the function registerFileResourceGetter(). If the resource is a SQL engine having a DBI connector defined, write a new DBIResourceConnector subclass and register an instance of it with the function registerDBIResourceConnector(). If the resource is in a domain specific web application or database, write a new ResourceResolver subclass and register an instance of it with the function registerResourceResolver(). This ResourceResolver object will create the appropriate ResourceClient object that matches your needs. The design of the URL that will describe your new resource should not overlap an existing one, otherwise the different registries will return the first instance for which the isFor(resource) is TRUE. In order to distinguish resource locations, the URL’s scheme can be extended, for instance the scheme for accessing a file in a Opal server is opal+https so that the credentials be applied as needed by Opal. 8.7 API declaration of a new resource It is possible to declare a resource that is to be resolved by an R package that uses the resourcer API. This figure illustrate how to declare a new reource called VCF2GDS that load Variant Calling Format files into Genomic Data Storage objects that can be managed whiting R. This will be further illustrated when analyzing omic data analysis in this Section. Figure 8.1: Declaration of a resource corresponding to a VCF2GDS format 8.8 Resource Forms As it can be error prone to define a new resource, when a URL is complex, or when there is a limited choice of formats or when credentials can be on different types, it is recommended to declare the resources forms and factory functions within the R package. This resource declaration is to be done in javascript, as this is a very commonly used language for building graphical user interfaces. These files are expected to be installed at the root of the package folder (then in the source code of the R package, they will be declared in the inst/resources folder), so that an external application can lookup statically the packages having declared some resources. The configuration file inst/resources/resource.js is a javascript file which contains an object with the properties: settings, a JSON object that contains the description and the documentation of the web forms (based on the json-schema specification). asResource, a javascript function that will convert the data captured from one of the declared web forms into a data structure representing the resource object. As an example (see also resourcer’s resource.js): var myPackage = { settings: { &quot;title&quot;: &quot;MyPackage resources&quot;, &quot;description&quot;: &quot;MyPackage resources are for etc.&quot;, &quot;web&quot;: &quot;https://github.com/org/myPackage&quot;, &quot;categories&quot;: [ { &quot;name&quot;: &quot;my-format&quot;, &quot;title&quot;: &quot;My data format&quot;, &quot;description&quot;: &quot;Data are files in my format, that will be read by myPackage etc.&quot; } ], &quot;types&quot;: [ { &quot;name&quot;: &quot;my-format-http&quot;, &quot;title&quot;: &quot;My data format - HTTP&quot;, &quot;description&quot;: &quot;Data are files in my format, that will be downloaded from a HTTP server etc.&quot;, &quot;tags&quot;: [&quot;my-format&quot;, &quot;http&quot;], &quot;parameters&quot;: {}, &quot;credentials&quot;: {} } ] }, asResource: function(type, name, params, credentials) { // make a resource object from arguments, using type to drive // what params/credentials properties are to be used // a basic example of resource object: return { &quot;name&quot;: name, &quot;url&quot;: params.url, &quot;format&quot;: params.format, &quot;identity&quot;: credentials.username, &quot;secret&quot;: credentials.password }; } } The specifications for the resource.js file are the following: settings object: Property Type Description title string The title of the set of resources. description string The description of the set of resources. web string A web link that describes the resources. categories array of object A list of category objects which are used to categorize the declared resources in terms of resource location, format, usage etc. types array of object A list of type objects which contains a description of the parameters and credentials forms for each type of resource. category object: Property Type Description name string The name of the category that will be applied to each resource type, must be unique. title string The title of the category. description string The description of the category. type object: Property Type Description name string The identifying name of the resource, must be unique. title string The title of the resource. description string The description of the resource form. tags array of string The tag names that are applied to the resource form. parameters object The form that will be used to capture the parameters to build the url and the format properties of the resource (based on the json-schema specification). Some specific fields can be used: _package to capture the R package name or _packages to capture an array of R package names to be loaded prior to the resource assignment. credentials object The form that will be used to capture the access credentials to build the identity and the secret properties of the resource (based on the json-schema specification). asResource function: a javascript function which signature is function(type, name, params, credentials) where: type, the form name used to capture the resource parameters and credentials, name, the name to apply to the resource, params, the captured parameters, credentials, the captured credentials. The name of the root object must follow the pattern: &lt;R package&gt; (note that any dots (.) in the R package name are to be replaced by underscores (_)). A real example of how to create shi file for the `{r Githubpkg(“isglobal-brge”, “dsOmics”)} package (described in this Section) can be found here 8.9 Examples with different resources Let us illustrate how to deal with different types of resources within DataSHIELD. To this end, let use our Opal test example available at https://opal-test.obiba.org which has the following reources Figure 8.2: Resources from a test enviroment available at https://opal-test.obiba.org Let us start by illustrating how to get a simple TSV file (brge.txt) into the R server. This file is located at a GitHub repository: https://raw.githubusercontent.com/isglobal-brge/brgedata/master/inst/extdata/brge.txt and it is not necesary to be moved from there. This is one of the main strenght of the resources implementation. 8.9.1 TSV file into a tibble or data.frame This code describes how to get the resource (a TSV file) as a data.frame into the R Server. Note that this is a secure access since user name and password must be provided library(DSI) library(DSOpal) library(dsBaseClient) # access to the &#39;brge&#39; resource (NOTE: test.brge is need since the project # is called test) builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-test.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;test.brge&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # the resource is loaded into R as the object &#39;res&#39; conns &lt;- DSI::datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # the resource is assigned to a data.frame # Assign to the original R class (e.g ExpressionSet) datashield.assign.expr(conns, symbol = &quot;dat&quot;, expr = quote(as.resource.data.frame(res))) ds.class(&quot;dat&quot;) $study1 [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; # logout the connection datashield.logout(conns) 8.9.2 .Rdata file into a specific R object Now let us describe how to get an specific type of R object into de R server. Our Opal test contains a resource called GSE80970 which is in a local machine. The resource is an R object of class ExpressionSet which is normally used to jointly capsulate gene expression, metadata and annotation. In general, we can retrieve any R object in their original format and if a method to coerce the specific object into a data.frame exists, we can also retrieve it as a tibble/data.frame. # prepare login data and resource to assign builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-test.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;test.GSE80970&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # login and assign resource (to &#39;res&#39; symbol) conns &lt;- DSI::datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # coerce ResourceClient objects to a data.frame called &#39;DF&#39; # NOTE: as.data.frame exists for `ExpressionSet` objects datashield.assign.expr(conns, symbol = &quot;DF&quot;, expr = quote(as.resource.data.frame(res))) ds.class(&quot;DF&quot;) $study1 [1] &quot;data.frame&quot; # we can also coerce ResourceClient objects to their original format. # This will allow the analyses with specific R/Bioconductor packages datashield.assign.expr(conns, symbol = &quot;ES&quot;, expr = quote(as.resource.object(res))) ds.class(&quot;ES&quot;) $study1 [1] &quot;ExpressionSet&quot; attr(,&quot;package&quot;) [1] &quot;Biobase&quot; # logout the connection datashield.logout(conns) "],
["omic-extension.html", "9 VCF files to GDS to peform GWAS with Bioconductor", " 9 VCF files to GDS to peform GWAS with Bioconductor Genomic data can be stored in different formats. PLINK and VCF files are commonly used in genetic epidemiology studies. In order to deal with this type of data, we have extended the resources available at the resourcer package to VCF files. NOTE: PLINK files can be translated into VCF files using different pipelines. In R you can use SeqArray to get VCF files. We use the Genomic Data Storage (GDS) format which efficiently manage VCF files into the R environment. This extension requires to create a Client and a Resolver function for the resourcer that are located into the dsOmics package. The client function uses snpgdsVCF2GDS function implemented in SNPrelate to coerce the VCF file to a GDS object. Then the GDS object is loaded into R as an object of class GdsGenotypeReader from GWASTools package that facilitates downstream analyses. The opal API server allows to incorporate this new type of resource as illustrated in Figure 9.1. Figure 9.1: Description of how a VCF file can be added to the opal resources It is important to notice that the URL should contain the tag method=biallelic.only&amp;snpfirstdim=TRUE since these are required parameters of snpgdsVCF2GDS function. This is an example: https://raw.githubusercontent.com/isglobal-brge/scoreInvHap/master/inst/extdata/example.vcf?method=biallelic.only&amp;snpfirstdim=TRUE In that case we indicate that only biallelic SNPs are considered (‘method=biallelic.only’) and that genotypes are stored in the individual-major mode, (i.e., list all SNPs for the first individual, and then list all SNPs for the second individual, etc) (‘snpfirstdim=TRUE’). "],
["shell-extension.html", "10 Secure shell programs: GWAS with PLINK", " 10 Secure shell programs: GWAS with PLINK GWAS can also be performed using program that are executed using shell commands. This is the case of PLINK, one of the state-of-the-art programs to run GWAS and other genomic data analyses such gene-enviroment interactions or polygenic risc score analyses that requires efficient and scalable pipelines. The resources also allow the use of secure SSH service to run programs on a remote server accessible through ssh containing data and analysis tools where R is just used for launching the analyses and aggregating results. This feature allow us to create functions to analyze data using specific shell programs. Here we describe how PLINK program can be use to perform GWAS [Yannick some overview about how this is created should be described] We use this following code to illustrate how analyses should be performed using the resourcer package. This code could be considered as the base code for creating a DataSHIELD package for the OPAL server as performed in plinkDS() function implemented in the dsOmics package We access the ssh resource called brge_plink (Figure 7.2) using the resourcer package as follows: library(resourcer) brge_plink &lt;- resourcer::newResource(url=&quot;ssh://plink-test.obiba.org:2222/home/master/brge?exec=ls,plink,plink1&quot;, identity = &quot;master&quot;, secret = &quot;master&quot;) client &lt;- resourcer::newResourceClient(brge_plink) This creates an object of this class: class(client) [1] &quot;SshResourceClient&quot; &quot;CommandResourceClient&quot; &quot;ResourceClient&quot; &quot;R6&quot; These are the actions we can do with an SshResourceClient object names(client) [1] &quot;.__enclos_env__&quot; &quot;clone&quot; &quot;close&quot; &quot;exec&quot; [5] &quot;removeTempDir&quot; &quot;tempDir&quot; &quot;uploadFile&quot; &quot;downloadFile&quot; [9] &quot;getConnection&quot; &quot;getAllowedCommands&quot; &quot;initialize&quot; &quot;asTbl&quot; [13] &quot;asDataFrame&quot; &quot;getResource&quot; For this specific resource (e.g. PLINK) we can execute these shell commands client$getAllowedCommands() [1] &quot;ls&quot; &quot;plink&quot; &quot;plink1&quot; For instance client$exec(&quot;ls&quot;, &quot;-la&quot;) $status [1] 0 $output [1] &quot;total 92992&quot; [2] &quot;drwxr-xr-x 2 master master 4096 Apr 29 09:50 .&quot; [3] &quot;drwxr-xr-x 6 master master 4096 May 3 18:00 ..&quot; [4] &quot;-rw-r--r-- 1 master master 57800003 Apr 29 09:50 brge.bed&quot; [5] &quot;-rw-r--r-- 1 master master 2781294 Apr 29 09:50 brge.bim&quot; [6] &quot;-rw-r--r-- 1 master master 45771 Apr 29 09:50 brge.fam&quot; [7] &quot;-rw-rw-r-- 1 master master 34442346 Apr 29 09:50 brge.gds&quot; [8] &quot;-rw-r--r-- 1 master master 59802 Apr 29 09:50 brge.phe&quot; [9] &quot;-rw-r--r-- 1 master master 72106 Apr 29 09:50 brge.txt&quot; $error character(0) $command [1] &quot;cd /home/master/brge &amp;&amp; ls -la&quot; attr(,&quot;class&quot;) [1] &quot;resource.exec&quot; Then, to avoid multiple accesses to the resource, it is recommended to create a temporal directory to save our results tempDir &lt;- client$tempDir() tempDir [1] &quot;/tmp/ssh-3050&quot; client$exec(&quot;ls&quot;, tempDir) $status [1] 0 $output character(0) $error character(0) $command [1] &quot;cd /home/master/brge &amp;&amp; ls /tmp/ssh-3050&quot; attr(,&quot;class&quot;) [1] &quot;resource.exec&quot; Then, we can use R to launch the shell commands client$exec(&#39;plink1&#39;, c(&#39;--bfile&#39;, &#39;brge&#39;, &#39;--freq&#39;, &#39;--out&#39;, paste0(tempDir, &#39;/out&#39;), &#39;--noweb&#39;)) $status [1] 0 $output [1] &quot;&quot; [2] &quot;@----------------------------------------------------------@&quot; [3] &quot;| PLINK! | v1.07 | 10/Aug/2009 |&quot; [4] &quot;|----------------------------------------------------------|&quot; [5] &quot;| (C) 2009 Shaun Purcell, GNU General Public License, v2 |&quot; [6] &quot;|----------------------------------------------------------|&quot; [7] &quot;| For documentation, citation &amp; bug-report instructions: |&quot; [8] &quot;| http://pngu.mgh.harvard.edu/purcell/plink/ |&quot; [9] &quot;@----------------------------------------------------------@&quot; [10] &quot;&quot; [11] &quot;Skipping web check... [ --noweb ] &quot; [12] &quot;Writing this text to log file [ /tmp/ssh-3050/out.log ]&quot; [13] &quot;Analysis started: Wed May 6 14:26:41 2020&quot; [14] &quot;&quot; [15] &quot;Options in effect:&quot; [16] &quot;\\t--bfile brge&quot; [17] &quot;\\t--freq&quot; [18] &quot;\\t--out /tmp/ssh-3050/out&quot; [19] &quot;\\t--noweb&quot; [20] &quot;&quot; [21] &quot;Reading map (extended format) from [ brge.bim ] &quot; [22] &quot;100000 markers to be included from [ brge.bim ]&quot; [23] &quot;Reading pedigree information from [ brge.fam ] &quot; [24] &quot;2312 individuals read from [ brge.fam ] &quot; [25] &quot;2312 individuals with nonmissing phenotypes&quot; [26] &quot;Assuming a disease phenotype (1=unaff, 2=aff, 0=miss)&quot; [27] &quot;Missing phenotype value is also -9&quot; [28] &quot;725 cases, 1587 controls and 0 missing&quot; [29] &quot;1097 males, 1215 females, and 0 of unspecified sex&quot; [30] &quot;Reading genotype bitfile from [ brge.bed ] &quot; [31] &quot;Detected that binary PED file is v1.00 SNP-major mode&quot; [32] &quot;Before frequency and genotyping pruning, there are 100000 SNPs&quot; [33] &quot;2312 founders and 0 non-founders found&quot; [34] &quot;6009 heterozygous haploid genotypes; set to missing&quot; [35] &quot;Writing list of heterozygous haploid genotypes to [ /tmp/ssh-3050/out.hh ]&quot; [36] &quot;7 SNPs with no founder genotypes observed&quot; [37] &quot;Warning, MAF set to 0 for these SNPs (see --nonfounders)&quot; [38] &quot;Writing list of these SNPs to [ /tmp/ssh-3050/out.nof ]&quot; [39] &quot;Writing allele frequencies (founders-only) to [ /tmp/ssh-3050/out.frq ] &quot; [40] &quot;&quot; [41] &quot;Analysis finished: Wed May 6 14:26:45 2020&quot; [42] &quot;&quot; $error character(0) $command [1] &quot;cd /home/master/brge &amp;&amp; plink1 --bfile brge --freq --out /tmp/ssh-3050/out --noweb&quot; attr(,&quot;class&quot;) [1] &quot;resource.exec&quot; The results can be retrieve as an R object outs &lt;- client$exec(&#39;ls&#39;, tempDir)$output outs [1] &quot;out.frq&quot; &quot;out.hh&quot; &quot;out.log&quot; &quot;out.nof&quot; client$downloadFile(paste0(tempDir, &#39;/out.*&#39;)) [1] &quot;./out.frq&quot; &quot;./out.hh&quot; &quot;./out.log&quot; &quot;./out.nof&quot; ans &lt;- readr::read_table(&quot;out.frq&quot;) ans # A tibble: 100,000 x 6 CHR SNP A1 A2 MAF NCHROBS &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 MitoC3993T T C 0.0149 4572 2 0 MitoG4821A A G 0.00175 4564 3 0 MitoG6027A A G 0.00434 4614 4 0 MitoT6153C C T 0.0106 4616 5 0 MitoC7275T T C 0.000866 4618 6 0 MitoT9699C C T 0.0732 4604 7 0 MitoA10045G G A 0.00780 4616 8 0 MitoG10311A A G 0.00261 4604 9 0 MitoA11252G G A 0.182 4518 10 0 MitoT11900C C T 0.000868 4608 # ... with 99,990 more rows Finally temporal directories are removed and session closed client$removeTempDir() client$close() "],
["setup.html", "11 Setup 11.1 Providing DataSHIELD packages in the opal server 11.2 Required R Packages in the client site (e.g. local machine)", " 11 Setup As describe in a previous Chapter the resourcer R package allows to deal with the main data sources (using tidyverse, DBI, dplyr, sparklyr, MongoDB, AWS S3, SSH etc.) and is easily extensible to new ones including specific data infrastructure in R or Bioconductor. So far ExpressionSet and RangedSummarizedExperiment objects saved in .rdata files are accesible through the resourcer package. The dsOmics package contains a new extension that deals with VCF (Variant Calling Format) files which are coerced to a GDS (Genomic Data Storage) format (VCF2GDS). In order to achive this resourcer extension, two R6 classes have been implemented: GDSFileResourceResolver class which handles file-base resources with data in GDS or VCF formats. This class is responsible for creating a GDSFileResourceClient object instance from an assigned resource. GDSFileResourceClient class which is responsible for getting the referenced file and making a connection (created by GWASTools) to the GDS file (will also convert the VCF file to a GDS file on the fly, using SNPRelate). For the subsequent analysis, it’s this connection handle to the GDS file that will be used. 11.1 Providing DataSHIELD packages in the opal server Required DataSHIELD packages must be uploaded in the opal server through the Administration site by accessing to DataSHIELD tab. In our case, both dsBase and dsOmics and resourcer packages must be installed as is illustrated in the figure. Figure 11.1: Installed packages in the test opal server The tab +Add package can be used to install a new package. The figure depicts how dsOmics was intalled into the opal server Figure 11.2: Description how dsOmics package was intalled into the test opal server For reproducing this book the following packages must be installed in the opal server From CRAN: - resourcer From Github: - datashield/dsBase - datashiled/dsGeo - isglobal-brge/dsOmics - tombiso/dsGeo 11.2 Required R Packages in the client site (e.g. local machine) Using DataSHIELD also requires some R packages to be install from the client site. So far, the following R packages must be installed (in their development version): devtools::install_github(&quot;obiba/opalr&quot;, dependencies = TRUE) devtools::install_github(&quot;datashield/DSI&quot;, dependencies = TRUE) devtools::install_github(&quot;datashield/DSOpal&quot;, dependencies = TRUE) devtools::install_github(&quot;datashield/dsBaseClient&quot;, ref = &quot;v6.0-dev&quot;, dependencies = TRUE) devtools::install_github(&quot;datashield/dsGeoClient&quot;, dependencies = TRUE) devtools::install_github(&quot;isglobal-brge/dsOmicsClient&quot;, dependencies = TRUE) devtools::install_github(&quot;tombisho/dsGeoClient&quot;, dependencies = TRUE) The package dependencies are then loaded as follows: library(DSI) library(DSOpal) library(dsBaseClient) library(dsOmicsClient) "],
["basic-statistical-analyses.html", "12 Basic statistical analyses 12.1 Analysis from a single study 12.2 Analysis from a multiple studies", " 12 Basic statistical analyses Let us start by illustrating how to peform simple statistical data analyses using different resources. Here, we will use data from three studies that are avaialbe in our OPAL test repository. The three databases are called CNSIM1, CNSIM2, CNSIM3 that are avaialble as three different resources: mySQL database, SPSS file and CSV file (see Figure 7.2). This example mimics real situations were different hospital or research centers manage their own databases containing harmonized data. Data correspond to three simulated datasets with different number of observations of 11 harmonized variables. They contain synthetic data based on a model derived from the participants of the 1958 Birth Cohort, as part of the obesity methodological development project. This dataset does contain some NA values. The available variables are: Variable Description Type Note LAB_TSC Total Serum Cholesterol numeric mmol/L LAB_TRIG Triglycerides numeric mmol/L LAB_HDL HDL Cholesterol numeric mmol/L LAB_GLUC_ADJUSTED Non-Fasting Glucose numeric mmol/L PM_BMI_CONTINUOUS Body Mass Index (continuous) numeric kg/m2 DIS_CVA History of Stroke factor 0 = Never had stroke; 1 = Has had stroke MEDI_LPD Current Use of Lipid Lowering Medication (from categorical assessment item) factor 0 = Not currently using lipid lowering medication; 1 = Currently using lipid lowering medication DIS_DIAB History of Diabetes factor 0 = Never had diabetes; 1 = Has had diabetes DIS_AMI History of Myocardial Infarction factor 0 = Never had myocardial infarction; 1 = Has had myocardial infarction GENDER Gender factor 0 = Female PM_BMI_CATEGORICAL Body Mass Index (categorical) factor 1 = Less than 25 kg/m2; 2 = 25 to 30 kg/m2; 3 = Over 30 kg/m2 The analyses that are described here, can also be found in the DataSHIELD Tutorial where these resources where uploaded into the Opal server as three tables, a much worse approach since data have to be moved from original repositories. 12.1 Analysis from a single study Let us start by illustrating how to analyze one data set (CNSIM1). library(DSI) library(DSOpal) library(dsBaseClient) # prepare login data and resource to assign builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-test.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;test.CNSIM1&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # login and assign resource conns &lt;- DSI::datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # coerce ResourceClient objects to a data.frame called &#39;D&#39; datashield.assign.expr(conns, symbol = &quot;D&quot;, expr = quote(as.resource.data.frame(res))) Then we can inspect the type of data we have ds.class(&quot;D&quot;) $study1 [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; ds.colnames(&quot;D&quot;) $study1 [1] &quot;entity_id&quot; &quot;DIS_AMI&quot; &quot;DIS_CVA&quot; &quot;DIS_DIAB&quot; [5] &quot;GENDER&quot; &quot;LAB_GLUC_ADJUSTED&quot; &quot;LAB_HDL&quot; &quot;LAB_TRIG&quot; [9] &quot;LAB_TSC&quot; &quot;MEDI_LPD&quot; &quot;PM_BMI_CATEGORICAL&quot; &quot;PM_BMI_CONTINUOUS&quot; Perform some data descriptive analyses ds.table1D(&quot;D$DIS_DIAB&quot;) $counts D$DIS_DIAB 0 3041 1 47 Total 3088 $percentages D$DIS_DIAB 0 98.48 1 1.52 Total 100.00 $validity [1] &quot;All tables are valid!&quot; ds.table2D(&quot;D$DIS_DIAB&quot;, &quot;D$GENDER&quot;) $colPercent $colPercent$`study1-D$DIS_DIAB(row)|D$GENDER(col)` 0 1 Total 0 98.04 98.94 98.48 1 1.96 1.06 1.52 Total 100.00 100.00 100.00 $colPercent.all.studies $colPercent.all.studies$`pooled-D$DIS_DIAB(row)|D$GENDER(col)` 0 1 Total 0 98.04 98.94 98.48 1 1.96 1.06 1.52 Total 100.00 100.00 100.00 $rowPercent $rowPercent$`study1-D$DIS_DIAB(row)|D$GENDER(col)` 0 1 Total 0 51.10 48.90 100 1 65.96 34.04 100 Total 51.33 48.67 100 $rowPercent.all.studies $rowPercent.all.studies$`pooled-D$DIS_DIAB(row)|D$GENDER(col)` 0 1 Total 0 51.10 48.90 100 1 65.96 34.04 100 Total 51.33 48.67 100 $chi2Test $chi2Test$`study1-D$DIS_DIAB(row)|D$GENDER(col)` Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: contingencyTable X-squared = 3.5158, df = 1, p-value = 0.06079 $chi2Test.all.studies $chi2Test.all.studies$`pooled-D$DIS_DIAB(row)|D$GENDER(col)` Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: pooledContingencyTable X-squared = 3.5158, df = 1, p-value = 0.06079 $counts $counts$`study1-D$DIS_DIAB(row)|D$GENDER(col)` 0 1 Total 0 1554 1487 3041 1 31 16 47 Total 1585 1503 3088 $counts.all.studies $counts.all.studies$`pooled-D$DIS_DIAB(row)|D$GENDER(col)` 0 1 Total 0 1554 1487 3041 1 31 16 47 Total 1585 1503 3088 $validity [1] &quot;All tables are valid!&quot; Or even some statistical modelling. In this case we want to assess whether sex (GENDER) or trigrycerids (LAB_TRIG) are risk factors for diabetes (DIS_DIAB) mod &lt;- ds.glm(DIS_DIAB ~ LAB_TRIG + GENDER, data = &quot;D&quot; , family=&quot;binomial&quot;) mod$coeff Estimate Std. Error z-value p-value low0.95CI.LP high0.95CI.LP P_OR (Intercept) -5.0254035 0.39487495 -12.726570 4.209299e-37 -5.7993442 -4.2514628 0.006526066 LAB_TRIG 0.3462886 0.09880155 3.504891 4.567950e-04 0.1526411 0.5399361 1.413810621 GENDER -0.4550068 0.39497696 -1.151983 2.493280e-01 -1.2291474 0.3191338 0.634443636 low0.95CI.P_OR high0.95CI.P_OR (Intercept) 0.00302039 0.01404336 LAB_TRIG 1.16490687 1.71589723 GENDER 0.29254188 1.37593539 As usual the connection must be closed datashield.logout(conns) 12.2 Analysis from a multiple studies library(DSOpal) library(dsBaseClient) # prepare login data and resources to assign builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-test.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;test.CNSIM1&quot;, driver = &quot;OpalDriver&quot;) builder$append(server = &quot;study2&quot;, url = &quot;https://opal-test.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;test.CNSIM2&quot;, driver = &quot;OpalDriver&quot;) builder$append(server = &quot;study3&quot;, url = &quot;https://opal-test.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;test.CNSIM3&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # login and assign resources conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # assigned objects are of class ResourceClient (and others) ds.class(&quot;res&quot;) $study1 [1] &quot;SQLResourceClient&quot; &quot;ResourceClient&quot; &quot;R6&quot; $study2 [1] &quot;TidyFileResourceClient&quot; &quot;FileResourceClient&quot; &quot;ResourceClient&quot; [4] &quot;R6&quot; $study3 [1] &quot;TidyFileResourceClient&quot; &quot;FileResourceClient&quot; &quot;ResourceClient&quot; [4] &quot;R6&quot; # coerce ResourceClient objects to data.frames # (DataSHIELD config allows as.resource.data.frame() assignment function for the purpose of the demo) datashield.assign.expr(conns, symbol = &quot;D&quot;, expr = quote(as.resource.data.frame(res))) ds.class(&quot;D&quot;) $study1 [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; $study2 [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; $study3 [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; # note that some dsBase functions do not like that the data.frame has multiple and different classes # (despite all are data.frames). Then query colnames one by one: lapply(conns, function(conn) {ds.colnames(&quot;D&quot;, datasources = conn)}) $study1 [1] &quot;id&quot; &quot;LAB_TSC&quot; &quot;LAB_TRIG&quot; &quot;LAB_HDL&quot; [5] &quot;LAB_GLUC_ADJUSTED&quot; &quot;PM_BMI_CONTINUOUS&quot; &quot;DIS_CVA&quot; &quot;MEDI_LPD&quot; [9] &quot;DIS_DIAB&quot; &quot;DIS_AMI&quot; &quot;GENDER&quot; &quot;PM_BMI_CATEGORICAL&quot; $study2 [1] &quot;entity_id&quot; &quot;DIS_AMI&quot; &quot;DIS_CVA&quot; &quot;DIS_DIAB&quot; [5] &quot;GENDER&quot; &quot;LAB_GLUC_ADJUSTED&quot; &quot;LAB_HDL&quot; &quot;LAB_TRIG&quot; [9] &quot;LAB_TSC&quot; &quot;MEDI_LPD&quot; &quot;PM_BMI_CATEGORICAL&quot; &quot;PM_BMI_CONTINUOUS&quot; $study3 [1] &quot;entity_id&quot; &quot;DIS_AMI&quot; &quot;DIS_CVA&quot; &quot;DIS_DIAB&quot; [5] &quot;GENDER&quot; &quot;LAB_GLUC_ADJUSTED&quot; &quot;LAB_HDL&quot; &quot;LAB_TRIG&quot; [9] &quot;LAB_TSC&quot; &quot;MEDI_LPD&quot; &quot;PM_BMI_CATEGORICAL&quot; &quot;PM_BMI_CONTINUOUS&quot; # do usual dsBase analysis ds.summary(&#39;D$LAB_HDL&#39;) $study1 $study1$class [1] &quot;numeric&quot; $study1$length [1] 2163 $study1$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 0.875240 1.047400 1.300000 1.581000 1.844500 2.090000 2.210900 1.569416 $study2 $study2$class [1] &quot;numeric&quot; $study2$length [1] 3088 $study2$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 0.850280 1.032200 1.294000 1.563000 1.840000 2.077000 2.225000 1.556648 $study3 $study3$class [1] &quot;numeric&quot; $study3$length [1] 4128 $study3$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 0.876760 1.039200 1.304000 1.589000 1.856000 2.098800 2.244200 1.574687 # vector types are not necessarily the same depending on the data reader that was used ds.class(&#39;D$GENDER&#39;) $study1 [1] &quot;integer64&quot; $study2 [1] &quot;haven_labelled&quot; $study3 [1] &quot;numeric&quot; ds.asFactor(&#39;D$GENDER&#39;, &#39;GENDER&#39;) $all.unique.levels [1] &quot;0&quot; &quot;1&quot; $return.message [1] &quot;Data object &lt;GENDER&gt; correctly created in all specified data sources&quot; ds.summary(&#39;GENDER&#39;) $study1 $study1$class [1] &quot;factor&quot; $study1$length [1] 2163 $study1$categories [1] &quot;0&quot; &quot;1&quot; $study1$`count of &#39;0&#39;` [1] 1092 $study1$`count of &#39;1&#39;` [1] 1071 $study2 $study2$class [1] &quot;factor&quot; $study2$length [1] 3088 $study2$categories [1] &quot;0&quot; &quot;1&quot; $study2$`count of &#39;0&#39;` [1] 1585 $study2$`count of &#39;1&#39;` [1] 1503 $study3 $study3$class [1] &quot;factor&quot; $study3$length [1] 4128 $study3$categories [1] &quot;0&quot; &quot;1&quot; $study3$`count of &#39;0&#39;` [1] 2091 $study3$`count of &#39;1&#39;` [1] 2037 # or coerce to a dplyr&#39;s tbl, which is more suitable for large/big datasets analysis # (DataSHIELD config allows as.resource.tbl() assignment function for the purpose of the demo) datashield.assign.expr(conns, symbol = &quot;T&quot;, expr = quote(as.resource.tbl(res))) ds.class(&quot;T&quot;) $study1 [1] &quot;tbl_MariaDBConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; [5] &quot;tbl&quot; $study2 [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; $study3 [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; # DataSHIELD analysis using dplyr objects and functions is to be invented... datashield.logout(conns) "],
["Omic.html", "13 Omic data analysis 13.1 Types of analyses implemented 13.2 Differential gene expression (DGE) analysis 13.3 Epigenome-wide association analysis (EWAS) 13.4 GWAS with Bioconductor 13.5 GWAS with PLINK", " 13 Omic data analysis In this part we will provide some real data anlyses of omic data including transcriptomic, epigenomic and genomic data that covers how to perform three of the widely used data analyses: differential gene expression (DGE), epigenome-wide association (EWAS) and genome-wide association (GWAS) analyses. We provide examples of how to perform data analyses using Bioconductor packages. For genomic data we also illustrate how to carry out analyses using PLINK. 13.1 Types of analyses implemented The Figure 13.1 describes the different types of omic association analyses that can be performed using DataSHIELD client functions implemented in the dsOmicsClient package. Basically, data (omic and phenotypes/covariates) can be stored in different sites (http, ssh, AWS S3, local, …) and are managed with Opal through the resourcer package and their extensions implemented in dsOmics. Figure 13.1: Non-disclosive omic data analysis with DataSHIELD and Bioconductor. The figure illustrates how the resourcer package is used to get access to omic data through the Opal servers. Then DataSHIELD is used in the client side to perform non-disclosive data analyses. Then, dsOmicsClient package allows different types of analyses: pooled and meta-analysis. Both methods are based on fitting different generalized linear models (GLMs) for each feature when assesing association between omic data and the phenotype/trait/condition of interest. Of course non-disclosive omic data analysis from a single study can also be performed. The pooled approach (Figure 13.2) is recommended when the user wants to analyze omic data from different sources and obtain results as if the data were located in a single computer. It should be noticed that this can be very time consuming when analyzing multiple features since it calls repeatedly to a base function in DataSHIELD (ds.glm) and that it cannot be recommended when data are not properly harmonized (e.g. gene expression normalized using different methods, GWAS data having different platforms, …). Also when it is necesary to remove unwanted variability (for transcriptomic and epigenomica analysis) or control for population stratification (for GWAS analysis), this approach cannot be used since we need to develop methods to compute surrogate variables (to remove unwanted variability) or PCAs (to to address population stratification) in a non-disclosive way. The meta-analysis approach Figure 13.3 overcomes the limitations raised when performing pooled analyses. First, the computation issue is addressed by using scalable and fast methods to perform data analysis at whole-genome level at each server. The transcriptomic and epigenomic data analyses make use of the widely used limma package that uses ExpressionSet or RangedSummarizedExperiment Bioc infrastructures to deal with omic and phenotypic (e.g covariates). The genomic data are analyzed using GWASTools and GENESIS that are designed to perform quality control (QC) and GWAS using GDS infrastructure. Next, we describe how both approaches are implemented: Pooled approach: Figure 13.2 illustrate how this analysis is performed. This corresponds to generalized linear models (glm) on data from single or multiple sources. It makes use of ds.glm() function which is a DataSHIELD function that uses an approach that is mathematically equivalent to placing all individual-level data froma all sources in one central warehouse and analysing those data using the conventional glm() function in R. The user can select one (or multiple) features (i.e., genes, transcripts, CpGs, SNPs, …) Figure 13.2: Non-disclosive omic data analysis with DataSHIELD and Bioconductor. The figure illustrates how to perform single pooled omic data analysis. The analyses are performed by using a generalized linear model (glm) on data from one or multiple sources. It makes use of ds.glm(), a DataSHIELD function, that uses an approach that is mathematically equivalent to placing all individual-level data from all sources in one central warehouse and analysing those data using the conventional glm() function in R. Meta-analysis: Figure 13.3 illustrate how this analysis is performed. This corresponds to perform a genome-wide analysis at each server using functions that are specifically design to that purpose and that are scalable. Then the results of each server can be meta-analyzed using method that meta-analyze either effects or p-values. Figure 13.3: Non-disclosive omic data analysis with DataSHIELD and Bioconductor. The figure illustrates how to perform anlyses at genome-wide level from one or multiple sources. It runs standard Bioconductor functions at each server independently to speed up the analyses and in the case of having multiple sources, results can be meta-analyzed uning standar R functions. 13.2 Differential gene expression (DGE) analysis Let us start by illustrating a simple example where a researcher may be interested in perfoming differential gene expression anaylis (DGE) having data in a single repository (e.g. one study). To this end, we will use bulk transcriptomic data from TCGA project. We have uploaded to the opal server a resource called tcga_liver whose URL is http://duffel.rail.bio/recount/TCGA/rse_gene_liver.Rdata which is available through the recount project. This resource contains the RangeSummarizedExperiment with the RNAseq profiling of liver cancer data from TCGA. Next, we illustrate how a differential expression analysis to compare RNAseq profiling of women vs men (variable gdc_cases.demographic.gender). The DGE analysis is normally performed using limma package. In that case, as we are analyzing RNA-seq data, limma + voom method will be required. Let us start by creating the connection to the opal server: builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-test.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;test.tcga_liver&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) Then, let us coerce the resource to a RangedSummarizedExperiment which is the type of object that is available in the recount project. datashield.assign.expr(conns, symbol = &quot;rse&quot;, expr = quote(as.resource.object(res))) ds.class(&quot;rse&quot;) $study1 [1] &quot;RangedSummarizedExperiment&quot; attr(,&quot;package&quot;) [1] &quot;SummarizedExperiment&quot; The number of features and samples can be inspected by ds.dim(&quot;rse&quot;) $`dimensions of rse in study1` [1] 58037 424 $`dimensions of rse in combined studies` [1] 58037 424 And the names of the features using the same function used in the case of analyzing an ExpressionSet name.features &lt;- ds.featureNames(&quot;rse&quot;) lapply(name.features, head) $study1 [1] &quot;ENSG00000000003.14&quot; &quot;ENSG00000000005.5&quot; &quot;ENSG00000000419.12&quot; &quot;ENSG00000000457.13&quot; [5] &quot;ENSG00000000460.16&quot; &quot;ENSG00000000938.12&quot; Also the covariate names can be inspected by name.vars &lt;- ds.featureData(&quot;rse&quot;) lapply(name.vars, head, n=15) $study1 [1] &quot;project&quot; [2] &quot;sample&quot; [3] &quot;experiment&quot; [4] &quot;run&quot; [5] &quot;read_count_as_reported_by_sra&quot; [6] &quot;reads_downloaded&quot; [7] &quot;proportion_of_reads_reported_by_sra_downloaded&quot; [8] &quot;paired_end&quot; [9] &quot;sra_misreported_paired_end&quot; [10] &quot;mapped_read_count&quot; [11] &quot;auc&quot; [12] &quot;sharq_beta_tissue&quot; [13] &quot;sharq_beta_cell_type&quot; [14] &quot;biosample_submission_date&quot; [15] &quot;biosample_publication_date&quot; We can visualize the levels of the variable having gender information ds.table1D(&quot;rse$gdc_cases.demographic.gender&quot;) $counts rse$gdc_cases.demographic.gender female 143 male 281 Total 424 $percentages rse$gdc_cases.demographic.gender female 33.73 male 66.27 Total 100.00 $validity [1] &quot;All tables are valid!&quot; The differential expression analysis is then performed by: ans.gender &lt;- ds.limma(model = ~ gdc_cases.demographic.gender, Set = &quot;rse&quot;, type.data = &quot;RNAseq&quot;, sva = FALSE) Notice that we have set type.data='RNAseq' to consider that our data are counts obtained from a RNA-seq experiment. By indicating so, the differential analysis is performed by using voom + limma as previously mention. As usual, we close the DataSHIELD session by: datashield.logout(conns) 13.3 Epigenome-wide association analysis (EWAS) EWAS requires basically the same statistical methods as those used in DGE. It should be notice that the pooled analysis we are going to illustrate here can also be performed with transcriptomic data since each study must have different range values. If so, gene expression harmonization should be performed, for instance, by standardizing the data at each study. For EWAS where methylation is measured using beta values (e.g CpG data are in the range 0-1) this is not a problem. In any case, adopting the meta-analysis approach could be a safe option. We have downloaded data from GEO corresponding to the accesion number GSE66351 which includes DNA methylation profiling (Illumina 450K array) of 190 individuals. Data corresponds to CpGs beta values measured in the superior temporal gyrus and prefrontal cortex brain regions of patients with Alzheimer’s. Data have been downloaded using GEOquery package that gets GEO data as ExpressionSet objects. Researchers who are not familiar with ExpressionSets can read this Section. Notice that data are encoded as beta-values that ensure data harmonization across studies. In order to illustrate how to perform data analyses using federated data, we have split the data into two ExpressionSets having 100 and 90 samples as if they were two different studies. Figure 7.2 shows the two resources defined for both studies (GSE66351_1 and GSE66351_2) In order to perform omic data analyses, we need first to login and assign resources to DataSHIELD. This can be performed using the as.resource.object() function builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-test.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;test.GSE66351_1&quot;, driver = &quot;OpalDriver&quot;) builder$append(server = &quot;study2&quot;, url = &quot;https://opal-test.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;test.GSE66351_2&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() conns &lt;- DSI::datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # Assign to the original R class (e.g ExpressionSet) datashield.assign.expr(conns, symbol = &quot;methy&quot;, expr = quote(as.resource.object(res))) Now, we can see that the resources are actually loaded into the R servers as their original class ds.class(&quot;methy&quot;) $study1 [1] &quot;ExpressionSet&quot; attr(,&quot;package&quot;) [1] &quot;Biobase&quot; $study2 [1] &quot;ExpressionSet&quot; attr(,&quot;package&quot;) [1] &quot;Biobase&quot; Then, some Bioconductor-type functions can be use to return non-disclosive information of ExpressionSets from each server to the client, using similar functions as those defined in the dsBaseClient package. For example, feature names can be returned by fn &lt;- ds.featureNames(&quot;methy&quot;) lapply(fn, head) $study1 [1] &quot;cg00000029&quot; &quot;cg00000108&quot; &quot;cg00000109&quot; &quot;cg00000165&quot; &quot;cg00000236&quot; &quot;cg00000289&quot; $study2 [1] &quot;cg00000029&quot; &quot;cg00000108&quot; &quot;cg00000109&quot; &quot;cg00000165&quot; &quot;cg00000236&quot; &quot;cg00000289&quot; Experimental phenotypes variables can be obtained by ds.varLabels(&quot;methy&quot;) $study1 [1] &quot;title&quot; &quot;geo_accession&quot; &quot;status&quot; [4] &quot;submission_date&quot; &quot;last_update_date&quot; &quot;type&quot; [7] &quot;channel_count&quot; &quot;source_name_ch1&quot; &quot;organism_ch1&quot; [10] &quot;characteristics_ch1&quot; &quot;characteristics_ch1.1&quot; &quot;characteristics_ch1.2&quot; [13] &quot;characteristics_ch1.3&quot; &quot;characteristics_ch1.4&quot; &quot;characteristics_ch1.5&quot; [16] &quot;characteristics_ch1.6&quot; &quot;characteristics_ch1.7&quot; &quot;characteristics_ch1.8&quot; [19] &quot;molecule_ch1&quot; &quot;extract_protocol_ch1&quot; &quot;label_ch1&quot; [22] &quot;label_protocol_ch1&quot; &quot;taxid_ch1&quot; &quot;hyb_protocol&quot; [25] &quot;scan_protocol&quot; &quot;description&quot; &quot;data_processing&quot; [28] &quot;platform_id&quot; &quot;contact_name&quot; &quot;contact_email&quot; [31] &quot;contact_phone&quot; &quot;contact_laboratory&quot; &quot;contact_institute&quot; [34] &quot;contact_address&quot; &quot;contact_city&quot; &quot;contact_zip/postal_code&quot; [37] &quot;contact_country&quot; &quot;supplementary_file&quot; &quot;supplementary_file.1&quot; [40] &quot;data_row_count&quot; &quot;age&quot; &quot;braak_stage&quot; [43] &quot;brain_region&quot; &quot;cell type&quot; &quot;diagnosis&quot; [46] &quot;donor_id&quot; &quot;sentrix_id&quot; &quot;sentrix_position&quot; [49] &quot;Sex&quot; $study2 [1] &quot;title&quot; &quot;geo_accession&quot; &quot;status&quot; [4] &quot;submission_date&quot; &quot;last_update_date&quot; &quot;type&quot; [7] &quot;channel_count&quot; &quot;source_name_ch1&quot; &quot;organism_ch1&quot; [10] &quot;characteristics_ch1&quot; &quot;characteristics_ch1.1&quot; &quot;characteristics_ch1.2&quot; [13] &quot;characteristics_ch1.3&quot; &quot;characteristics_ch1.4&quot; &quot;characteristics_ch1.5&quot; [16] &quot;characteristics_ch1.6&quot; &quot;characteristics_ch1.7&quot; &quot;characteristics_ch1.8&quot; [19] &quot;molecule_ch1&quot; &quot;extract_protocol_ch1&quot; &quot;label_ch1&quot; [22] &quot;label_protocol_ch1&quot; &quot;taxid_ch1&quot; &quot;hyb_protocol&quot; [25] &quot;scan_protocol&quot; &quot;description&quot; &quot;data_processing&quot; [28] &quot;platform_id&quot; &quot;contact_name&quot; &quot;contact_email&quot; [31] &quot;contact_phone&quot; &quot;contact_laboratory&quot; &quot;contact_institute&quot; [34] &quot;contact_address&quot; &quot;contact_city&quot; &quot;contact_zip/postal_code&quot; [37] &quot;contact_country&quot; &quot;supplementary_file&quot; &quot;supplementary_file.1&quot; [40] &quot;data_row_count&quot; &quot;age&quot; &quot;braak_stage&quot; [43] &quot;brain_region&quot; &quot;cell type&quot; &quot;diagnosis&quot; [46] &quot;donor_id&quot; &quot;sentrix_id&quot; &quot;sentrix_position&quot; [49] &quot;Sex&quot; attr(,&quot;class&quot;) [1] &quot;dsvarLabels&quot; &quot;list&quot; 13.3.1 Single CpG analysis Once the methylation data have been loaded into the opal server, we can perform different type of analyses using functions from the dsOmicsClient package. Let us start by illustrating how to analyze a single CpG from two studies by using an approach that is mathematically equivalent to placing all individual-level. ans &lt;- ds.lmFeature(feature = &quot;cg07363416&quot;, model = ~ diagnosis + Sex, Set = &quot;methy&quot;, datasources = conns) ans Estimate Std. Error p-value cg07363416 0.03459886 0.02504291 0.1670998 attr(,&quot;class&quot;) [1] &quot;dsLmFeature&quot; &quot;matrix&quot; 13.3.2 Multiple CpG analysis The same analysis can be performed for all features (e.g. CpGs) just avoiding the feature argument. This process can be parallelized using mclapply function from the multicore package. ans &lt;- ds.lmFeature(model = ~ diagnosis + Sex, Set = &quot;methy&quot;, datasources = conns, mc.cores = 20) This method corresponds to the pooled analysis approach and can be very time consiming since the function repeatedly calls the DataSHIELD function ds.glm(). We can adopt another strategy that is to run a glm of each feature independently at each study using limma package (which is really fast) and then combine the results (i.e. meta-analysis approach). ans.limma &lt;- ds.limma(model = ~ diagnosis + Sex, Set = &quot;methy&quot;, datasources = conns) Then, we can visualize the top genes at each study (i.e server) by lapply(ans.limma, head) $study1 # A tibble: 6 x 10 id logFC CI.L CI.R AveExpr t P.Value adj.P.Val B SE &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg13138089 -0.147 -0.191 -0.103 0.380 -6.62 0.00000000190 0.000466 10.6 0.0122 2 cg23859635 -0.0569 -0.0741 -0.0397 0.200 -6.58 0.00000000232 0.000466 10.4 0.00520 3 cg13772815 -0.0820 -0.107 -0.0570 0.437 -6.50 0.00000000327 0.000466 10.0 0.0135 4 cg12706938 -0.0519 -0.0678 -0.0359 0.145 -6.45 0.00000000425 0.000466 9.76 0.00872 5 cg24724506 -0.0452 -0.0593 -0.0312 0.139 -6.39 0.00000000547 0.000466 9.51 0.00775 6 cg02812891 -0.125 -0.165 -0.0860 0.247 -6.33 0.00000000731 0.000466 9.23 0.0163 $study2 # A tibble: 6 x 10 id logFC CI.L CI.R AveExpr t P.Value adj.P.Val B SE &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg04046629 -0.101 -0.135 -0.0669 0.345 -5.91 0.0000000621 0.0172 7.18 0.0128 2 cg07664323 -0.0431 -0.0577 -0.0284 0.776 -5.85 0.0000000822 0.0172 6.90 0.00390 3 cg27098804 -0.0688 -0.0924 -0.0452 0.277 -5.79 0.000000107 0.0172 6.64 0.0147 4 cg08933615 -0.0461 -0.0627 -0.0296 0.166 -5.55 0.000000298 0.0360 5.64 0.00791 5 cg18349298 -0.0491 -0.0671 -0.0311 0.157 -5.42 0.000000507 0.0489 5.12 0.00848 6 cg02182795 -0.0199 -0.0272 -0.0125 0.0947 -5.36 0.000000670 0.0538 4.84 0.0155 The annotation can be added by using the argument annotCols. It should be a vector with the columns of the annotation available in the ExpressionSet or RangedSummarizedExperiment that want to be showed. The columns of the annotation can be obtained by ds.fvarLabels(&quot;methy&quot;) $study1 [1] &quot;ID&quot; &quot;Name&quot; &quot;AddressA_ID&quot; [4] &quot;AlleleA_ProbeSeq&quot; &quot;AddressB_ID&quot; &quot;AlleleB_ProbeSeq&quot; [7] &quot;Infinium_Design_Type&quot; &quot;Next_Base&quot; &quot;Color_Channel&quot; [10] &quot;Forward_Sequence&quot; &quot;Genome_Build&quot; &quot;CHR&quot; [13] &quot;MAPINFO&quot; &quot;SourceSeq&quot; &quot;Chromosome_36&quot; [16] &quot;Coordinate_36&quot; &quot;Strand&quot; &quot;Probe_SNPs&quot; [19] &quot;Probe_SNPs_10&quot; &quot;Random_Loci&quot; &quot;Methyl27_Loci&quot; [22] &quot;UCSC_RefGene_Name&quot; &quot;UCSC_RefGene_Accession&quot; &quot;UCSC_RefGene_Group&quot; [25] &quot;UCSC_CpG_Islands_Name&quot; &quot;Relation_to_UCSC_CpG_Island&quot; &quot;Phantom&quot; [28] &quot;DMR&quot; &quot;Enhancer&quot; &quot;HMM_Island&quot; [31] &quot;Regulatory_Feature_Name&quot; &quot;Regulatory_Feature_Group&quot; &quot;DHS&quot; [34] &quot;RANGE_START&quot; &quot;RANGE_END&quot; &quot;RANGE_GB&quot; [37] &quot;SPOT_ID&quot; $study2 [1] &quot;ID&quot; &quot;Name&quot; &quot;AddressA_ID&quot; [4] &quot;AlleleA_ProbeSeq&quot; &quot;AddressB_ID&quot; &quot;AlleleB_ProbeSeq&quot; [7] &quot;Infinium_Design_Type&quot; &quot;Next_Base&quot; &quot;Color_Channel&quot; [10] &quot;Forward_Sequence&quot; &quot;Genome_Build&quot; &quot;CHR&quot; [13] &quot;MAPINFO&quot; &quot;SourceSeq&quot; &quot;Chromosome_36&quot; [16] &quot;Coordinate_36&quot; &quot;Strand&quot; &quot;Probe_SNPs&quot; [19] &quot;Probe_SNPs_10&quot; &quot;Random_Loci&quot; &quot;Methyl27_Loci&quot; [22] &quot;UCSC_RefGene_Name&quot; &quot;UCSC_RefGene_Accession&quot; &quot;UCSC_RefGene_Group&quot; [25] &quot;UCSC_CpG_Islands_Name&quot; &quot;Relation_to_UCSC_CpG_Island&quot; &quot;Phantom&quot; [28] &quot;DMR&quot; &quot;Enhancer&quot; &quot;HMM_Island&quot; [31] &quot;Regulatory_Feature_Name&quot; &quot;Regulatory_Feature_Group&quot; &quot;DHS&quot; [34] &quot;RANGE_START&quot; &quot;RANGE_END&quot; &quot;RANGE_GB&quot; [37] &quot;SPOT_ID&quot; attr(,&quot;class&quot;) [1] &quot;dsfvarLabels&quot; &quot;list&quot; Then we can run the analysis and obtain the output with the chromosome and gene symbol by: ans.limma.annot &lt;- ds.limma(model = ~ diagnosis + Sex, Set = &quot;methy&quot;, annotCols = c(&quot;CHR&quot;, &quot;UCSC_RefGene_Name&quot;), datasources = conns) lapply(ans.limma.annot, head) $study1 # A tibble: 6 x 12 id CHR UCSC_RefGene_Na~ logFC CI.L CI.R AveExpr t P.Value adj.P.Val B SE &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg13~ 2 &quot;ECEL1P2&quot; -0.147 -0.191 -0.103 0.380 -6.62 1.90e-9 0.000466 10.6 0.0122 2 cg23~ 2 &quot;MTA3&quot; -0.0569 -0.0741 -0.0397 0.200 -6.58 2.32e-9 0.000466 10.4 0.00520 3 cg13~ 17 &quot;&quot; -0.0820 -0.107 -0.0570 0.437 -6.50 3.27e-9 0.000466 10.0 0.0135 4 cg12~ 19 &quot;MEX3D&quot; -0.0519 -0.0678 -0.0359 0.145 -6.45 4.25e-9 0.000466 9.76 0.00872 5 cg24~ 19 &quot;ISOC2;ISOC2;IS~ -0.0452 -0.0593 -0.0312 0.139 -6.39 5.47e-9 0.000466 9.51 0.00775 6 cg02~ 2 &quot;ECEL1P2&quot; -0.125 -0.165 -0.0860 0.247 -6.33 7.31e-9 0.000466 9.23 0.0163 $study2 # A tibble: 6 x 12 id CHR UCSC_RefGene_Na~ logFC CI.L CI.R AveExpr t P.Value adj.P.Val B SE &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg04~ 11 &quot;CD6&quot; -0.101 -0.135 -0.0669 0.345 -5.91 6.21e-8 0.0172 7.18 0.0128 2 cg07~ 6 &quot;MUC21&quot; -0.0431 -0.0577 -0.0284 0.776 -5.85 8.22e-8 0.0172 6.90 0.00390 3 cg27~ 11 &quot;CD6&quot; -0.0688 -0.0924 -0.0452 0.277 -5.79 1.07e-7 0.0172 6.64 0.0147 4 cg08~ 1 &quot;&quot; -0.0461 -0.0627 -0.0296 0.166 -5.55 2.98e-7 0.0360 5.64 0.00791 5 cg18~ 3 &quot;RARRES1;RARRES~ -0.0491 -0.0671 -0.0311 0.157 -5.42 5.07e-7 0.0489 5.12 0.00848 6 cg02~ 8 &quot;&quot; -0.0199 -0.0272 -0.0125 0.0947 -5.36 6.70e-7 0.0538 4.84 0.0155 Then, the last step is to meta-analyze the results. Different methods can be used to this end. We have implemented a method that meta-analyze the p-pvalues of each study as follows: ans.meta &lt;- metaPvalues(ans.limma) ans.meta # A tibble: 481,868 x 2 pvals$id $study1 $study2 p.meta &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg13138089 0.00000000190 0.00000763 4.78e-13 2 cg25317941 0.0000000179 0.00000196 1.12e-12 3 cg02812891 0.00000000731 0.00000707 1.63e-12 4 cg12706938 0.00000000425 0.0000161 2.14e-12 5 cg16026647 0.000000101 0.000000797 2.51e-12 6 cg12695465 0.00000000985 0.0000144 4.33e-12 7 cg21171625 0.000000146 0.00000225 9.78e-12 8 cg13772815 0.00000000327 0.000122 1.18e-11 9 cg00228891 0.000000166 0.00000283 1.38e-11 10 cg21488617 0.0000000186 0.0000299 1.62e-11 # ... with 481,858 more rows This is a genreal method that can be used … We can verify that the results are pretty similar to those obtained using pooled analyses. Here we compute the association for two of the top-CpGs: res1 &lt;- ds.lmFeature(feature = &quot;cg13138089&quot;, model = ~ diagnosis + Sex, Set = &quot;methy&quot;, datasources = conns) res1 Estimate Std. Error p-value cg13138089 -0.1373348 0.01712405 1.057482e-15 attr(,&quot;class&quot;) [1] &quot;dsLmFeature&quot; &quot;matrix&quot; res2 &lt;- ds.lmFeature(feature = &quot;cg13772815&quot;, model = ~ diagnosis + Sex, Set = &quot;methy&quot;, datasources = conns) res2 Estimate Std. Error p-value cg13772815 -0.06786137 0.009128915 1.056225e-13 attr(,&quot;class&quot;) [1] &quot;dsLmFeature&quot; &quot;matrix&quot; We can create a QQ-plot by using the generic function plot (here not showed). In some cases inflation can be observed, so that, correction for cell-type or surrogate variables must be performed. We describe how we can do that in the next two sections. 13.3.3 Adjusting for Surrogate Variables The vast majority of omic studies require to control for unwanted variability. The surrogate variable analysis (SVA) can address this issue by estimating some hidden covariates that capture differences across individuals due to some artifacts such as batch effects or sample quality sam among others. The method is implemented in SVA package. Performing this type of analysis using the ds.lmFeature function is not allowed since estimating SVA would require to implement a non-disclosive method that computes SVA from the different servers. This will be a future topic of the dsOmicsClient. NOTE that, estimating SVA separately at each server would not be a good idea since the aim of SVA is to capture differences mainly due to experimental issues among ALL individuals. What we can do instead is to use the ds.limma function to perform the analyses adjusted for SVA at each study. ans.sva &lt;- ds.limma(model = ~ diagnosis + Sex, Set = &quot;methy&quot;, sva = TRUE, annotCols = c(&quot;CHR&quot;, &quot;UCSC_RefGene_Name&quot;)) ans.sva $study1 # A tibble: 481,868 x 12 id CHR UCSC_RefGene_Na~ logFC CI.L CI.R AveExpr t P.Value adj.P.Val B &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg10~ 19 &quot;GNG7&quot; -0.0547 -0.0721 -0.0373 0.338 -6.25 1.31e-8 0.00591 8.51 2 cg13~ 17 &quot;&quot; -0.0569 -0.0757 -0.0381 0.437 -6.00 3.91e-8 0.00591 7.43 3 cg11~ 19 &quot;PODNL1;PODNL1;~ 0.0334 0.0223 0.0445 0.568 5.97 4.53e-8 0.00591 7.29 4 cg27~ 17 &quot;SLC47A2;SLC47A~ 0.0274 0.0182 0.0365 0.548 5.95 4.91e-8 0.00591 7.21 5 cg21~ 6 &quot;&quot; -0.0453 -0.0609 -0.0297 0.799 -5.78 1.05e-7 0.00666 6.46 6 cg23~ 2 &quot;MTA3&quot; -0.0327 -0.0440 -0.0215 0.200 -5.77 1.09e-7 0.00666 6.42 7 cg10~ 16 &quot;SALL1;SALL1&quot; 0.0366 0.0240 0.0492 0.137 5.77 1.10e-7 0.00666 6.42 8 cg24~ 1 &quot;&quot; 0.0317 0.0208 0.0427 0.821 5.76 1.11e-7 0.00666 6.41 9 cg13~ 9 &quot;&quot; -0.0326 -0.0440 -0.0213 0.367 -5.72 1.32e-7 0.00705 6.24 10 cg13~ 2 &quot;ECEL1P2&quot; -0.106 -0.144 -0.0686 0.380 -5.61 2.15e-7 0.00835 5.76 # ... with 481,858 more rows, and 1 more variable: SE &lt;dbl&gt; $study2 # A tibble: 481,868 x 12 id CHR UCSC_RefGene_Na~ logFC CI.L CI.R AveExpr t P.Value adj.P.Val B &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg16~ 12 &quot;LRP1&quot; -0.0418 -0.0548 -0.0289 0.388 -6.42 7.82e-9 0.00374 9.05 2 cg25~ 1 &quot;&quot; -0.0664 -0.0874 -0.0453 0.569 -6.27 1.55e-8 0.00374 8.37 3 cg12~ 11 &quot;NRXN2;NRXN2&quot; -0.0273 -0.0367 -0.0179 0.728 -5.81 1.13e-7 0.0149 6.42 4 cg07~ 3 &quot;&quot; -0.0393 -0.0529 -0.0258 0.160 -5.78 1.24e-7 0.0149 6.32 5 cg07~ 6 &quot;MUC21&quot; -0.0427 -0.0579 -0.0276 0.776 -5.61 2.63e-7 0.0163 5.59 6 cg00~ 1 &quot;CR1L&quot; -0.0568 -0.0771 -0.0365 0.350 -5.56 3.18e-7 0.0163 5.40 7 cg11~ 12 &quot;CNTN1;CNTN1&quot; -0.0443 -0.0601 -0.0284 0.152 -5.56 3.18e-7 0.0163 5.40 8 cg03~ 7 &quot;PGAM2;PGAM2&quot; -0.0442 -0.0600 -0.0283 0.211 -5.56 3.26e-7 0.0163 5.38 9 cg07~ 1 &quot;KCNAB2;KCNAB2&quot; 0.0611 0.0392 0.0831 0.659 5.54 3.47e-7 0.0163 5.31 10 cg25~ 17 &quot;WNK4&quot; -0.0392 -0.0533 -0.0251 0.512 -5.52 3.70e-7 0.0163 5.25 # ... with 481,858 more rows, and 1 more variable: SE &lt;dbl&gt; attr(,&quot;class&quot;) [1] &quot;dsLimma&quot; &quot;list&quot; Then, data can be combined meta-anlyzed as follows: ans.meta.sv &lt;- metaPvalues(ans.sva) ans.meta.sv # A tibble: 481,868 x 2 pvals$id $study1 $study2 p.meta &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 cg00228891 0.00000397 0.000000318 3.58e-11 2 cg01301319 0.00000609 0.00000123 2.00e-10 3 cg22962123 0.00000106 0.00000767 2.17e-10 4 cg24302412 0.0000139 0.000000762 2.77e-10 5 cg02812891 0.000000408 0.0000327 3.47e-10 6 cg23859635 0.000000109 0.000190 5.29e-10 7 cg13138089 0.000000215 0.000105 5.74e-10 8 cg24938077 0.0000125 0.00000254 8.02e-10 9 cg13772815 0.0000000391 0.00132 1.27e- 9 10 cg21212881 0.000000340 0.000248 2.04e- 9 # ... with 481,858 more rows The DataSHIELD session must by closed by: datashield.logout(conns) 13.4 GWAS with Bioconductor We have a GWAS example available at BRGE data repository that aims to find SNPs associated with asthma. We have genomic data in a VCF file (brge.vcf) along with several covariates and phenotypes in the file brge.txt (gender, age, obesity, smoking, country and asthma status). The same data is also available in PLINK format (brge.bed, brge.bim, brge.fam) with covariates in the file brge.phe. We have created a resource having the VCF file of our study on asthma as previously described. The name of the resource is brge_vcf the phenotypes are available in another resource called brge that is a .txt file (see 7.2). The GWAS analysis is then perform as follows. We first start by preparing login data builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-test.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;test.brge_vcf&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) In this case we have to assign to different resources. One for the VCF (obesity_vcf) and another one for the phenotypic data (obesity). To this end, the datashield.assign.resource function is required before assigning any object to the specific resource. Notice that the VCF resource can be load into R as a GDS thanks to our extension of existing resources in the reourcer datashield.assign.resource(conns, symbol = &quot;vcf.res&quot;, resource = list(study1 = &quot;test.brge_vcf&quot;)) datashield.assign.expr(conns, symbol = &quot;gds&quot;, expr = quote(as.resource.object(vcf.res))) datashield.assign.resource(conns, symbol = &quot;covars.res&quot;, resource = list(study1 = &quot;test.brge&quot;)) datashield.assign.expr(conns, symbol = &quot;covars&quot;, expr = quote(as.resource.data.frame(covars.res))) These are the objects available in the Opal server ds.ls() $study1 $study1$environment.searched [1] &quot;R_GlobalEnv&quot; $study1$objects.found [1] &quot;covars&quot; &quot;covars.res&quot; &quot;gds&quot; &quot;res&quot; &quot;vcf.res&quot; We can use dsBaseClient functions to inspect the variables that are in the covars data.frame. The variables are ds.colnames(&quot;covars&quot;) $study1 [1] &quot;scanID&quot; &quot;gender&quot; &quot;obese&quot; &quot;age&quot; &quot;smoke&quot; &quot;country&quot; &quot;asthma&quot; The asthma variable has this number of individuals at each level (1: controls, 2: cases) ds.table1D(&quot;covars$asthma&quot;) $counts covars$asthma 0 1587 1 725 Total 2312 $percentages covars$asthma 0 68.64 1 31.36 Total 100.00 $validity [1] &quot;All tables are valid!&quot; Then, an object of class GenotypeData must be created at the server side to perform genetic data analyses. This is a container defined in the GWASTools package for storing genotype and phenotypic data from genetic association studies. By doing that we will also verify whether individuals in the GDS (e.g VCF) and covariates files have the same individuals and are in the same order. This can be performed by ds.GenotypeData(x=&#39;gds&#39;, covars = &#39;covars&#39;, columnId = 1, newobj.name = &#39;gds.Data&#39;) The association analysis for a given SNP is performed by simply ds.glmSNP(snps.fit = &quot;rs11247693&quot;, model = asthma ~ gender + age, genoData=&#39;gds.Data&#39;) Estimate Std. Error p-value rs11247693 -0.1543215 0.2309585 0.5040196 attr(,&quot;class&quot;) [1] &quot;dsGlmSNP&quot; &quot;matrix&quot; The analysis of all available SNPs is performed when the argument snps.fit is missing. The function performs the analysis of the selected SNPs in a single repository or in multiple repositories as performing pooled analyses (it uses ds.glm DataSHIELD function). As in the case of transcriptomic data, analyzing all the SNPs in the genome (e.g GWAS) will be high time-consuming. We can adopt a similar approach as the one adopted using the limma at each server. That is, we run GWAS at each repository using specific and scalable packages available in R/Bioc. In that case we use the GWASTools and GENESIS packages. The complete pipeline is implemented in this function ans.bioC &lt;- ds.GWAS(&#39;gds.Data&#39;, model=asthma~age+country) This close the DataSHIELD session datashield.logout(conns) 13.5 GWAS with PLINK Here we illustrate how to perform the same GWAS analyses on the asthma using PLINK secure shell commands. This can be performed thanks to the posibility of having ssh resources as described here. It is worth to notice that this workflow and the new R functions implemented in dsOmicsClient could be used as a guideline to carry out similar analyses using existing analysis tools in genomics such as IMPUTE, SAMtools or BEDtools among many others. We start by assigning login resources library(DSOpal) library(dsBaseClient) library(dsOmicsClient) builder &lt;- newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-test.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;test.brge_plink&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() Then we assign the resource to a symbol (i.e. R object) called client which is a ssh resource conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;client&quot;) ds.class(&quot;client&quot;) $study1 [1] &quot;SshResourceClient&quot; &quot;CommandResourceClient&quot; &quot;ResourceClient&quot; &quot;R6&quot; Now, we are ready to run any PLINK command from the client site. Notice that in this case we want to assess association between the genotype data in bed format and use as phenotype the variable ‘obese’ that is in the file ‘obesity.phe’. The sentence in a PLINK command would be (NOTE: we avoid –out to indicate the output file since the file will be available in R as a tibble). plink --bfile obesity --assoc --pheno obesity.phe --pheno-name obese The arguments musth be encapsulated in a single character without the command ‘plink’ plink.arguments &lt;- &quot;--bfile brge --logistic --covar brge.phe --covar-name gender,age&quot; the analyses are then performed by ans.plink &lt;- ds.PLINK(&quot;client&quot;, plink.arguments) The object ans contains the PLINK results at each server as well as the outuput provided by PLINK lapply(ans.plink, names) $study1 [1] &quot;results&quot; &quot;plink.out&quot; head(ans.plink$study1$results) # A tibble: 6 x 9 CHR SNP BP A1 TEST NMISS OR STAT P &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 MitoC3993T 3993 T ADD 2286 0.752 -1.33 0.182 2 0 MitoC3993T 3993 T gender 2286 0.742 -3.27 0.00107 3 0 MitoC3993T 3993 T age 2286 1.00 0.565 0.572 4 0 MitoG4821A 4821 A ADD 2282 2.68 1.71 0.0879 5 0 MitoG4821A 4821 A gender 2282 0.740 -3.31 0.000940 6 0 MitoG4821A 4821 A age 2282 1.00 0.465 0.642 ans.plink$study$plink.out $status [1] 0 $output [1] &quot;&quot; [2] &quot;@----------------------------------------------------------@&quot; [3] &quot;| PLINK! | v1.07 | 10/Aug/2009 |&quot; [4] &quot;|----------------------------------------------------------|&quot; [5] &quot;| (C) 2009 Shaun Purcell, GNU General Public License, v2 |&quot; [6] &quot;|----------------------------------------------------------|&quot; [7] &quot;| For documentation, citation &amp; bug-report instructions: |&quot; [8] &quot;| http://pngu.mgh.harvard.edu/purcell/plink/ |&quot; [9] &quot;@----------------------------------------------------------@&quot; [10] &quot;&quot; [11] &quot;Skipping web check... [ --noweb ] &quot; [12] &quot;Writing this text to log file [ /tmp/ssh-3621/out.log ]&quot; [13] &quot;Analysis started: Wed May 6 13:36:51 2020&quot; [14] &quot;&quot; [15] &quot;Options in effect:&quot; [16] &quot;\\t--bfile brge&quot; [17] &quot;\\t--logistic&quot; [18] &quot;\\t--covar brge.phe&quot; [19] &quot;\\t--covar-name gender,age&quot; [20] &quot;\\t--noweb&quot; [21] &quot;\\t--out /tmp/ssh-3621/out&quot; [22] &quot;&quot; [23] &quot;Reading map (extended format) from [ brge.bim ] &quot; [24] &quot;100000 markers to be included from [ brge.bim ]&quot; [25] &quot;Reading pedigree information from [ brge.fam ] &quot; [26] &quot;2312 individuals read from [ brge.fam ] &quot; [27] &quot;2312 individuals with nonmissing phenotypes&quot; [28] &quot;Assuming a disease phenotype (1=unaff, 2=aff, 0=miss)&quot; [29] &quot;Missing phenotype value is also -9&quot; [30] &quot;725 cases, 1587 controls and 0 missing&quot; [31] &quot;1097 males, 1215 females, and 0 of unspecified sex&quot; [32] &quot;Reading genotype bitfile from [ brge.bed ] &quot; [33] &quot;Detected that binary PED file is v1.00 SNP-major mode&quot; [34] &quot;Reading 6 covariates from [ brge.phe ] with nonmissing values for 2199 individuals&quot; [35] &quot;Selected subset of 2 from 6 covariates&quot; [36] &quot;For these, nonmissing covariate values for 2312 individuals&quot; [37] &quot;Before frequency and genotyping pruning, there are 100000 SNPs&quot; [38] &quot;2312 founders and 0 non-founders found&quot; [39] &quot;6009 heterozygous haploid genotypes; set to missing&quot; [40] &quot;Writing list of heterozygous haploid genotypes to [ /tmp/ssh-3621/out.hh ]&quot; [41] &quot;7 SNPs with no founder genotypes observed&quot; [42] &quot;Warning, MAF set to 0 for these SNPs (see --nonfounders)&quot; [43] &quot;Writing list of these SNPs to [ /tmp/ssh-3621/out.nof ]&quot; [44] &quot;Total genotyping rate in remaining individuals is 0.994408&quot; [45] &quot;0 SNPs failed missingness test ( GENO &gt; 1 )&quot; [46] &quot;0 SNPs failed frequency test ( MAF &lt; 0 )&quot; [47] &quot;After frequency and genotyping pruning, there are 100000 SNPs&quot; [48] &quot;After filtering, 725 cases, 1587 controls and 0 missing&quot; [49] &quot;After filtering, 1097 males, 1215 females, and 0 of unspecified sex&quot; [50] &quot;Converting data to Individual-major format&quot; [51] &quot;Writing logistic model association results to [ /tmp/ssh-3621/out.assoc.logistic ] &quot; [52] &quot;&quot; [53] &quot;Analysis finished: Wed May 6 13:38:48 2020&quot; [54] &quot;&quot; $error character(0) $command [1] &quot;cd /home/master/brge &amp;&amp; plink1 --bfile brge --logistic --covar brge.phe --covar-name gender,age --noweb --out /tmp/ssh-3621/out&quot; attr(,&quot;class&quot;) [1] &quot;resource.exec&quot; We can compare the p-values obtained using PLINK with Bioconductor-based packages for the top-10 SNPs as follows: library(tidyverse) # get SNP p.values (additive model - ADD) res.plink &lt;- ans.plink$study1$results %&gt;% filter(TEST==&quot;ADD&quot;) %&gt;% arrange(P) # compare top-10 with Biocoductor&#39;s results snps &lt;- res.plink$SNP[1:10] plink &lt;- res.plink %&gt;% filter(SNP%in%snps) %&gt;% select(SNP, P) bioC &lt;- ans.bioC$study1 %&gt;% filter(rs%in%snps) %&gt;% select(rs, Score.pval) left_join(plink, bioC, by=c(&quot;SNP&quot; = &quot;rs&quot;)) # A tibble: 10 x 3 SNP P Score.pval &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 rs2267914 0.00000151 0.000000809 2 rs6097326 0.00000424 0.00000642 3 rs7153 0.00000440 0.00000508 4 rs3732410 0.00000817 0.00000940 5 rs7995146 0.0000170 0.0000195 6 rs6495788 0.0000213 0.0000278 7 rs1602679 0.0000268 0.0000264 8 rs11055608 0.0000270 0.0000161 9 rs7098143 0.0000313 0.0000214 10 rs7676164 0.0000543 0.0000537 As expected, the p-values are in the same order of magnitud having little variations due to the implemented methods of each software. We can do the same comparions of minor allele frequency (MAF) estimation performed with Bioconductor and PLINK. To this end, we need first to estimate MAF using PLINK plink.arguments &lt;- &quot;--bfile brge --freq&quot; ans.plink2 &lt;- ds.PLINK(&quot;client&quot;, plink.arguments) maf.plink &lt;- ans.plink2$study1$results plink &lt;- maf.plink %&gt;% filter(SNP%in%snps) %&gt;% select(SNP, MAF) bioC &lt;- ans.bioC$study1 %&gt;% filter(rs%in%snps) %&gt;% select(rs, freq) left_join(plink, bioC, by=c(&quot;SNP&quot; = &quot;rs&quot;)) # A tibble: 10 x 3 SNP MAF freq &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 rs7153 0.256 0.256 2 rs3732410 0.254 0.254 3 rs7676164 0.304 0.304 4 rs1602679 0.101 0.101 5 rs7098143 0.210 0.210 6 rs11055608 0.446 0.446 7 rs7995146 0.0527 0.0527 8 rs6495788 0.267 0.267 9 rs2267914 0.104 0.104 10 rs6097326 0.125 0.125 This close the DataSHIELD session datashield.logout(conns) "],
["GIS.html", "14 Geographical data analysis", " 14 Geographical data analysis "],
["dslite-datashield-implementation-on-local-datasets.html", "15 DSLite: DataSHIELD Implementation on Local Datasets 15.1 Development Environment Setup 15.2 DataSHIELD Development Flow 15.3 DataSHIELD Sessions 15.4 Debugging 15.5 Limitations", " 15 DSLite: DataSHIELD Implementation on Local Datasets DSLite is a serverless DataSHIELD Interface (DSI) implementation which purpose is to mimic the behavior of a distant (virtualized or barebone) data repository server (see DSOpal for instance). The datasets that are being analyzed must be fully accessible in the local environment and then the non-disclosive constraint of the analysis is not relevant for DSLite: some DSLite functionalities allows to inspect what is under the hood of the DataSHIELD computation nodes, making it a perfect tool for DataSHIELD analysis package developers. 15.1 Development Environment Setup 15.1.1 DataSHIELD Packages Both client and server side packages must be installed in your local R session. The entry point is still the client side package and DSLite will automatically load the corresponding server side package on DataSHIELD aggregate and assignment functions call, based on the DataSHIELD configuration. The minimum required packages should be: install.packages(&quot;resourcer&quot;, dependencies = TRUE) devtools::install_github(&quot;datashield/DSLite&quot;, dependencies = TRUE) devtools::install_github(&quot;datashield/dsBase&quot;, ref = &quot;v6.0-dev&quot;, dependencies = TRUE) 15.1.2 Test Datasets DSLite comes with a set of datasets that can be easily loaded. You can also provide your own to illustrate a specific data analysis function. 15.1.3 R Package Development Tools We recommend using the following tools to facilitate R package development: devtools, the collection of package development tools, usethis, automate package and project setup tasks that are otherwise performed manually, testthat, for unit testing, roxygen2, for writing documentation in-line with code, Rstudio, the R editor that integrates the tools mentioned above and more. 15.2 DataSHIELD Development Flow The typical development flow, using DSLite, is: Build and install your client and/or server side DataSHIELD packages. Create a new DSLiteServer object instance, refering test datasets. Use or alter the default DataSHIELD configuration. Test your DataSHIELD client/server functions. Debug DataSHIELD server nodes using DSLiteServer methods. 15.2.1 DSLiteServer After your client and/or server side DataSHIELD packages have been built and installed, a new DSLiteServer object instance must be created. Some DSLiteServer methods can be used to verify or modify the DSLiteServer behaviour: DSLiteServer$strict() DSLiteServer$home() See the R documentation of the DSLiteServer class for details. As an example: library(DSLite) # prepare test data in a light DS server data(&quot;CNSIM1&quot;) data(&quot;CNSIM2&quot;) data(&quot;CNSIM3&quot;) dslite.server &lt;- newDSLiteServer(tables=list(CNSIM1=CNSIM1, CNSIM2=CNSIM2, CNSIM3=CNSIM3)) # load corresponding DataSHIELD login data data(&quot;logindata.dslite.cnsim&quot;) The previous example can be simplified using the set-up functions based on the provided test datasets: library(DSLite) # load CNSIM test data logindata.dslite.cnsim &lt;- setupCNSIMTest() 15.2.2 DataSHIELD Configuration The DataSHIELD configuration (aggregate and assign functions, R options) is automatically discovered by inspecting the R packages installed and having some DataSHIELD settings defined, either in their DESCRIPTION file or in a DATASHIELD file. This default configuration extracting function is: DSLite::defaultDSConfiguration() The list of the DataSHIELD R packages to be inspected (or excluded) when building the default configuration can be specified as parameters of defaultDSConfiguration(). The DataSHIELD configuration can be specified at DSLiteServer creation time or afterwards with some DSLiteServer methods that can be used to verify or modify the DSLiteServer configuration: DSLiteServer$config() DSLiteServer$aggregateMethods() DSLiteServer$aggregateMethod() DSLiteServer$assignMethods() DSLiteServer$assignMethod() DSLiteServer$options() DSLiteServer$option() See the R documentation of the DSLiteServer class for details. As an example: # verify configuration dslite.server$config() 15.3 DataSHIELD Sessions The following figure illustrates a setup where a single DSLiteServer holds several data frames and is used by two different DataSHIELD Connection (DSConnection) objects. All these objects live in the same R environment (usually the Global Environment). The “server” is responsible for managing DataSHIELD sessions that are implemented as distinct R environments inside of which R symbols are assigned and R functions are evaluated. Using the R environment paradigm ensures that the different DataSHIELD execution context (client and servers) are contained and exclusive from each other. DSLite architecture After performing the login DataSHIELD phase, the DSLiteServer holds the different DataSHIELD server side sessions, i.e. R environments identified by an ID. These IDs are also stored within the DataSHIELD connection objects that are the result of the datashield.login() call. The folllowing example shows how to access these session IDs: # datashield logins and assignments conns &lt;- datashield.login(logindata.dslite.cnsim, assign=TRUE) # get the session ID of &quot;sim1&quot; node connection object conns$sim1@sid # the same ID is in the DSLiteServer dslite.server$hasSession(conns$sim1@sid) 15.4 Debugging Thanks to the DSLiteServer capability to have its configuration modified at any time, it is possible to add some debugging functions without polluting in the DataSHIELD package you are developping. For instance, this code adds an aggregate function print(): # add a print method to configuration dslite.server$aggregateMethod(&quot;print&quot;, function(x){ print(x) }) # and use it to print the D symbol datashield.aggregate(conns, quote(print(D))) Another option is to get a symbol value from the server into the client environment. This can be very helpful for complex data structures. The following example illustrates usage of a shortcut function that iterates over all the connection objects and get the corresponding symbol value: # get data represented by symbol D for each DataSHIELD connection data &lt;- getDSLiteData(conns, &quot;D&quot;) # get data represented by symbol D from a specific DataSHIELD connection data1 &lt;- getDSLiteData(conns$sim1, &quot;D&quot;) 15.5 Limitations 15.5.1 Function Parameters Parser The main difference with a regular DSI implementation (such as the one of DSOpal) is that the arguments of the DataSHIELD functional calls are not parsed in DSLite. The only R language element that is inspected and handled is the name of the functions, that are replaced by the ones defined in the DataSHIELD configuration. For instance the following expression, which includes a function call in the formula, is valid for the DSLiteServer but not for Opal: someregression(D$height ~ D$diameter + poly(D$length,3,raw=TRUE)) As a consequence, DataSHIELD R package development can take advantage of DSLite flexibility for speeding development but will never replace testing on a regular DataSHIELD infrastructure using DSOpal. 15.5.2 Server Side Environments For each of the DataSHIELD node, the server side code is evaluated within an environment that has no parent, i.e. detached from the global environment where the client code is executed. Some R functions have a parameter that allows to specify to which environment they apply, for instance assign(), get(), eval(), as.formula(), etc. Their env (or envir) parameter default value is parent.frame() which is the global environment when executed in Opal’s R server, because it is the parent frame of the package’s namespace where the function is defined. In DSLiteServer, the parent frame must be the environment where the server code is evaluated. In order to be consistent between these two execution contexts (Opal R server and DSLiteServer), you must specify the env (or envir) value explicitly to be parent.frame(), which is the parent frame of the block being executed (either the global environment in Opal context, or the environment defined in DSLiteServer). Example of a valid server side piece of code that assigns a value to a symbol in the DataSHIELD server’s environment (being the Opal R server’s global environment or a DSLiteServer’s environment): base::assign(x = &quot;D&quot;, value = someValue, envir = parent.frame()) See also the Advanced R - Environments documentation to learn more about environments. "],
["creating-datashield-packages.html", "16 Creating DataSHIELD packages", " 16 Creating DataSHIELD packages "],
["tips-and-tricks.html", "17 Tips and tricks 17.1 How to install R packages into OPAL server from R 17.2 How to check whether there are open R sesions in the OPAL server 17.3 How", " 17 Tips and tricks 17.1 How to install R packages into OPAL server from R 17.2 How to check whether there are open R sesions in the OPAL server 17.3 How "],
["session-info.html", "18 Session Info", " 18 Session Info sessionInfo() R version 3.6.3 (2020-02-29) Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 10 x64 (build 18362) Matrix products: default locale: [1] LC_COLLATE=Spanish_Spain.1252 LC_CTYPE=Spanish_Spain.1252 LC_MONETARY=Spanish_Spain.1252 [4] LC_NUMERIC=C LC_TIME=Spanish_Spain.1252 attached base packages: [1] stats graphics grDevices utils datasets methods base other attached packages: [1] dsOmicsClient_0.3.0 dsBaseClient_6.0.0-03 DSOpal_1.1.0 DSI_1.1.0 [5] R6_2.4.1 progress_1.2.2 opalr_1.3.0 httr_1.4.1 loaded via a namespace (and not attached): [1] Rcpp_1.0.4.6 mvtnorm_1.1-0 lattice_0.20-38 prettyunits_1.1.1 [5] png_0.1-7 mutoss_0.1-12 zoo_1.8-7 utf8_1.1.4 [9] assertthat_0.2.1 digest_0.6.25 mime_0.9 stats4_3.6.3 [13] evaluate_0.14 highr_0.8 pillar_1.4.3 TFisher_0.2.0 [17] Rdpack_0.11-1 rlang_0.4.5 curl_4.3 multcomp_1.4-13 [21] rstudioapi_0.11 Matrix_1.2-18 rmarkdown_2.1 splines_3.6.3 [25] metap_1.3 stringr_1.4.0 compiler_3.6.3 numDeriv_2016.8-1.1 [29] xfun_0.13 pkgconfig_2.0.3 BiocGenerics_0.32.0 multtest_2.42.0 [33] mnormt_1.5-6 htmltools_0.4.0 tidyselect_0.2.5 tibble_2.1.3 [37] bookdown_0.18 codetools_0.2-16 fansi_0.4.1 crayon_1.3.4 [41] dplyr_0.8.3 MASS_7.3-51.5 grid_3.6.3 jsonlite_1.6.1 [45] magrittr_1.5 bibtex_0.4.2.2 cli_2.0.2 stringi_1.4.6 [49] sn_1.6-1 vctrs_0.2.4 sandwich_2.5-1 BiocStyle_2.14.4 [53] TH.data_1.0-10 tools_3.6.3 Biobase_2.44.0 glue_1.3.1 [57] purrr_0.3.3 hms_0.5.3 plotrix_3.7-8 parallel_3.6.3 [61] survival_3.1-11 yaml_2.2.1 BiocManager_1.30.10 gbRd_0.4-11 [65] knitr_1.28 "],
["bibliography.html", "19 Bibliography", " 19 Bibliography "]
]
