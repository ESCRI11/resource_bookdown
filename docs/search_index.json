[
["index.html", "Orchestrating non-disclosive big data analyses of shared data from different resources with R and DataSHIELD Wellcome", " Orchestrating non-disclosive big data analyses of shared data from different resources with R and DataSHIELD 2020-05-05 Wellcome This is the website for “non-disclosive big data analyses of shared data from different resources with R and DataSHIELD”, a book that provides users with some common workflows for the non-disclosive analysis of biomedical data with R and DataSHIELD from different resources. This book will teach you how to use the resourcer to perform any statistcal analysis of data from different studies and having data in different formats (e.g., CSV, SPSS, R class, …). In particular, we focus on illustrating how to deal with Big Data by providing several examples from omic and geographical settings. We use cutting-edge Bioconductor tools to perform transcriptomic, epigenomic and genomic data analyses. Serveral R packages are used to perform analysis of geospatial data. We also provide examples of how performing non-disclosive analyses using secure SHELL commands by allowing the use of specific software that properly deals with Big Data outside R. This material serves as an online companion for the manuscript “Orchestrating non-disclosive big data analyses of shared data from different resources with R and DataSHIELD”. While we focus here genomic and geoespatial data, dozens of data analysis aplications interested in performing non-disclosive analysis having data in any specific format could be carried out. By learning the grammar of DataSHILED workflows, we hope to provide you a starting point for the exploration of your own data, whether it be omic, geospatial or otherwise. This book is organized into four parts. In the Preamble, we introduce the book and provides a tutorial for key data infrastructure useful for omic and geospatial data that are used throughout omic association and GIS analyses. The second part, Focus Topics, dive into information for learning DataSHIELD (we assume that users already know R). This part includes an overview of the framework for non-disclosive analyses using any resource which is one of the key advances provided in this work. The third part, Workflows, provides primarily code detailing the analysis of various datasets throughout the book. The fourth part, Developers, provides information about how to develop DataSHIELD packages and some useful answers to questions that developers or end users may face whe using our proposed infrastructure. Finally, the Appendix highlights our contributors. If you would like to cite this work, please use the reference “Orchestrating non-disclosive big data analyses of shared data from different resources with R and DataSHIELD”. "],
["introduction.html", "1 Introduction", " 1 Introduction "],
["rbioconductor-data-infrastructures.html", "2 R/Bioconductor data infrastructures", " 2 R/Bioconductor data infrastructures "],
["opal.html", "3 OPAL", " 3 OPAL "],
["datashield.html", "4 DataSHIELD 4.1 Overview", " 4 DataSHIELD 4.1 Overview DataSHIELD infrastructure is a software solution that allows simultaneous co-analysis of data from multiple studies stored on different servers without the need to physically pool data or disclose sensitive information. DataSHIELD uses Opal servers to properly perform such analyses. At a high level DataSHIELD is set up as a client-server model which houses the data for a particular study. A request is made from the client to run specific functions on the remote servers where the analysis is performed. Non-sensitive and pre-approved summary statistics are returned from each study to the client where they can be combined for an overall analysis. An overview of what a single-site DataSHIELD architecture would look like is illustrated in Figure 4.1. Figure 4.1: Single Server DataSHIELD Architecture (Wilson et al 2017) "],
["dsi-datashield-interface-implementation-for-opal-data-repository.html", "5 DSI: DataSHIELD Interface implementation for Opal data repository 5.1 Class Structure 5.2 Higher Level Functions 5.3 Options", " 5 DSI: DataSHIELD Interface implementation for Opal data repository The DataSHIELD Interface (DSI) defines a set of S4 classes and generic methods that can be implemented for accessing a data repository supporting the DataSHIELD infrastructure: controlled R commands to be executed on the server side are garanteeing that non disclosive information is returned to client side. 5.1 Class Structure The DSI classes are: DSObject a common base class for all DSI, DSDriver drives the creation of a connection object, DSConnection allows the interaction with the remote server; DataSHIELD operations such as aggregation and assignment return a result object; DataSHIELD setup status check can be performed (dataset access, configuration comparision), DSResult wraps access to the result, which can be fetched either synchronously or asynchronously depending on the capabilities of the data repository server. All classes are virtual: they cannot be instantiated directly and instead must be subclassed. See DSOpal for a reference implementation of DSI based on the Opal data repository. These S4 classes and generic methods are meant to be used for implementing connection to a DataSHIELD-aware data repository. 5.2 Higher Level Functions In addition to these S4 classes, DSI provides functions to handle a list of remote data repository servers: datashield.login and datashield.logout will make use of the DSDriver paradigm to create DSConnections to the data repositories, datashield.aggregate and datashield.assign will perform typical DataSHIELD operations on DSConnections, which result will be fetched through DSResult objects, datashield.connections, datashield.connections_default and datashield.connections_find are functions for managing the list of DSConnection objects that will be discovered and used by the client-side analytic functions. Other data management functions are provided by the DSConnection objects: datashield.workspaces, datashield.workspace_save and datashield.workspace_rm allow to manage R images of the remote DataSHIELD sessions (to speed up data analysis sessions), datashield.symbols and datashield.symbol_rm offer a minimalistic management of the R symbols living in the remote DataSHIELD sessions, datashield.table_status, datashield.pkg_status, datashield.method_status and datashield.methods are utility functions to explore the DataSHIELD setup across a set of data repositories, These datashield.* functions are meant to be used by DataSHIELD packages developers and users. 5.3 Options Some options can be set to modify the behavior of the DSI: datashield.env is the R environment in which the DSConnection object list is to be looking for. Default value is the Global Environment: globalenv(). datashield.progress is a logical to enable the visibility of the progress bars. Default value is TRUE. datashield.progress.clear is a logical to make the progress bar disappear after it has been completed. Default value is FALSE. "],
["the-resources.html", "6 The resources 6.1 Introduction 6.2 Types of resources", " 6 The resources 6.1 Introduction Resources are datasets or computation units which location is described by a URL and access is protected by credentials. When assigned to a R/DataSHIELD server session, remote big/complex datasets or high performance computers are made accessible to data analysts. Instead of storing the data in Opal’s database, only the way to access them is to be defined: the datasets are kept in their original format and location (a SQL database, a SPSS file, R object, etc.) and are read directly from the R/DataSHIELD server-side session. Then as soon as there is a R reader for the dataset or a connector for the analysis services, a resource can be defined. Opal takes care of the DataSHIELD permissions (a DS user cannot see the resource’s credentials) and of the resources assignment to a R/DataSHIELD session (see Figure 6.1) Figure 6.1: Resources: a new DataSHIELD infrastructure 6.2 Types of resources The data format refers to the intrinsic structure of the data. A very common family of data formats is the tabular format which is made of rows (entities, records, observations etc.) and columns (variables, fields, vectors etc.). Examples of tabular formats are the delimiter-separated values formats (CSV, TSV etc.), the spreadsheet data formats (Microsoft Excel, LibreOffice Calc, Google Sheets etc.), some proprietary statistical software data formats (SPSS, SAS, Stata etc.), the database tables that can be stored in structured database management systems that are row-oriented (MySQL, MariaDB, PostgreSQL, Oracle, SQLite etc.) or column-oriented (Apache Cassandra, Apache Parquet, MariaDB ColumnStore, BigTable etc.), or in semi-structured database management systems such as the documented-oriented databases (MongoDB, Redis, CouchDB, Elasticsearch etc.). When the data model is getting complex (data types and objects relationships), a domain-specific data format is sometimes designed to handle this complexity so that statistical analysis and data retrieval can be executed as efficiently as possible. Examples of domain-specific data formats are encountered in the omic or geospatial fields of research that are described in the Workflows part: Omic and Geospatial. A data format can also include some additional features such as data compression, encoding or encryption. Each data format requires an appropriate reader software library or application to extract the information or perform data aggregation or filtering operations. We have prepared a test environment, with the Opal implementation of Resources and an appropriate R/DataSHIELD configuration that is available at: opal-test.obiba.org. This figure illustrate the resources which are available for the test project and can serve as a starting example of the different types of resources that can be dealt with Figure 6.2: Resources from a test enviroment available at https://opal-test.obiba.org As it can be seen, the data storage can simply be a file to be accessed directly from the host’s file system or to be downloaded from a remote location. More advanced data storage systems are software applications that expose an interface to query, extract or analyse the data. These applications can make use of a standard programming interface (e.g. SQL) or expose specific web services (e.g. based on the HTTP communication protocol) or provide a software library (in different programming languages) to access the data. These different ways of accessing the data are not exclusive from each other. In some cases the micro-data cannot be extracted, only computation services that return aggregated data are provided. The data storage system can also apply security rules, requiring authentication and proper authorisations to access or analyse the data. We call resource this data or computation access description. A resource will have the following properties: (1) the location of the data or of the computation services, (2) the data format (if this information cannot be inferred from the location property), (3) the access credentials (if some apply). The resource location description will make use of the web standard described in the RFC 3986 “Uniform Resource Identifier (URI): Generic Syntax”. More specifically, the Uniform Resource Locator (URL) specification is what we need for defining the location of the data or computation resource: the term Uniform allows to describe the resource the same way, independently of its type, location and usage context; the term Resource does not limit the scope of what might be a resource, e.g. a document, a service, a collection of resources, or even abstract concepts (operations, relationships, etc.); the term Locator both identifies the resource and provides a means of locating it by describing its access mechanism (e.g. the network location). The URL syntax is composed of several parts: (1) a scheme, that describes how to access the resource, e.g. the communication protocols “https” (secured HTTP communication), “ssh” (secured shell, for issuing commands on a remote server), or “s3” (for accessing Amazon Web Service S3 file store services), (2) an authority (optional), e.g. a server name address, (3) a path that identifies/locates the resource in a hierarchical way and that can be altered by query parameters. The resource’s data format might be inferred from the path part of the URL, by using the file name suffix for instance. Nevertheless, sometimes it is not possible to identify the data format because the path could make sense only for the data storage system, for example when a file store designates a document using an obfuscated string identifier or when a text-based data format is compressed as a zip archive. The format property can provide this information. Despite the authority part of the URL can contain some user information (such as the username and password), it is discouraged to use this capability for security considerations. The resource’s credentials property will be used instead, and will be composed of an identifier sub-property and a secret sub-property, which can be used for authenticating with a username/password, or an access token, or any other credentials encoded string. The advantage of separating the credentials property from the resource location property is that a user with limited permissions could have access to the resource’s location information while the credentials are kept secret. Once a resource has been formally defined, it should be possible to build programmatically a connection object that will make use of the described data or computation services. This resource description is not bound to a specific programmatic language (the URL property is a web standard, other properties are simple strings) and does not enforce the use of a specific software application for building, storing and interpreting a resource object. Next Section describes the resourcer package which is an R implementation of the data and computation resources description and connection. There the reader can see some examples of how dealing with different resources in DataSHIELD. "],
["resourcer.html", "7 The resourcer package 7.1 File Resources 7.2 Database Resources 7.3 Computation Resources 7.4 Extending Resources 7.5 API declaration of a new resource 7.6 Examples with different resources", " 7 The resourcer package The resourcer package is meant to access resources identified by a URL in a uniform way whether it references a dataset (stored in a file, a SQL table, a MongoDB collection etc.) or a computation unit (system commands, web services etc.). Usually some credentials will be defined, and an additional data format information can be provided to help dataset coercing to a data.frame object. The main concepts are: Resource, access to a resource (dataset or computation unit) is described by an object with URL, optional credentials and optional data format properties, ResourceResolver, a ResourceClient factory based on the URL scheme and available in a resolvers registry, ResourceClient, realizes the connection with the dataset or the computation unit described by a Resource, FileResourceGetter, connect to a file described by a resource, DBIResourceConnector, establish a DBI connection. 7.1 File Resources These are resources describing a file. If the file is in a remote location, it must be downloaded before being read. The data format specification of the resource helps to find the appropriate file reader. 7.1.1 File Getter The file locations supported by default are: file, local file system, http(s), web address, basic authentication, gridfs, MongoDB file store, scp, file copy through SSH, opal, Opal file store. This can be easily applied to other file locations by extending the FileResourceGetter class. An instance of the new file resource getter is to be registered so that the FileResourceResolver can operate as expected. resourcer::registerFileResourceGetter(MyFileLocationResourceGetter()$new()) 7.1.2 File Data Format The data format specified within the Resource object, helps at finding the appropriate file reader. Currently supported data formats are: the data formats that have a reader in tidyverse: readr (csv, csv2, tsv, ssv, delim), haven (spss, sav, por, dta, stata, sas, xpt), readxl (excel, xls, xlsx). This can be easily applied to other data file formats by extending the FileResourceClient class. the R data format that can be loaded in a child R environment from which object of interest will be retrieved. Usage example that reads a local SPSS file: # make a SPSS file resource res &lt;- resourcer::newResource( name = &quot;CNSIM1&quot;, url = &quot;file:///data/CNSIM1.sav&quot;, format = &quot;spss&quot; ) # coerce the csv file in the opal server to a data.frame df &lt;- as.data.frame(res) To support other file data format, extend the FileResourceClient class with the new data format reader implementation. Associate factory class, an extension of the ResourceResolver class is also to be implemented and registered. resourcer::registerResourceResolver(MyFileFormatResourceResolver$new()) 7.2 Database Resources 7.2.1 DBI Connectors DBI is a set of virtual classes that are are used to abstract the SQL database connections and operations within R. Then any DBI implementation can be used to access to a SQL table. Which DBI connector to be used is an information that can be extracted from the scheme part of the resource’s URL. For instance a resource URL starting with postgres:// will require the RPostgres driver. To separate the DBI connector instanciation from the DBI interface interactions in the SQLResourceClient, a DBIResourceConnector registry is to be populated. The currently supported SQL database connectors are: mariadb MariaDB connector, mysql MySQL connector, postgres or postgresql Postgres connector, presto, presto+http or presto+https Presto connector, spark, spark+http or spark+https Spark connector. To support another SQL database having a DBI driver, extend the DBIResourceConnector class and register it: resourcer::registerDBIResourceConnector(MyDBResourceConnector$new()) 7.2.2 Use dplyr Having the data stored in the database allows to handle large (common SQL databases) to big (PrestoDB, Spark) datasets using dplyr which will delegate as much as possible operations to the database. 7.2.3 Document Databases NoSQL databases can be described by a resource. The nodbi can be used here. Currently only connection to MongoDB database is supported using URL scheme mongodb or mongodb+srv. 7.3 Computation Resources Computation resources are resources on which tasks/commands can be triggerred and from which resulting data can be retrieved. Example of computation resource that connects to a server through SSH: # make an application resource on a ssh server res &lt;- resourcer::newResource( name = &quot;supercomp1&quot;, url = &quot;ssh://server1.example.org/work/dir?exec=plink,ls&quot;, identity = &quot;sshaccountid&quot;, secret = &quot;sshaccountpwd&quot; ) # get ssh client from resource object client &lt;- resourcer::newResourceClient(res) # does a ssh::ssh_connect() # execute commands files &lt;- client$exec(&quot;ls&quot;) # exec &#39;cd /work/dir &amp;&amp; ls&#39; # release connection client$close() # does ssh::ssh_disconnect(session) 7.4 Extending Resources There are several ways to extend the Resources handling. These are based on different R6 classes having a isFor(resource) function: If the resource is a file located at a place not already handled, write a new FileResourceGetter subclass and register an instance of it with the function registerFileResourceGetter(). If the resource is a SQL engine having a DBI connector defined, write a new DBIResourceConnector subclass and register an instance of it with the function registerDBIResourceConnector(). If the resource is in a domain specific web application or database, write a new ResourceResolver subclass and register an instance of it with the function registerResourceResolver(). This ResourceResolver object will create the appropriate ResourceClient object that matches your needs. The design of the URL that will describe your new resource should not overlap an existing one, otherwise the different registries will return the first instance for which the isFor(resource) is TRUE. In order to distinguish resource locations, the URL’s scheme can be extended, for instance the scheme for accessing a file in a Opal server is opal+https so that the credentials be applied as needed by Opal. 7.5 API declaration of a new resource It is possible to declare a resource that is to be resolved by an R package that uses the resourcer API. This figure illustrate how to declare a new reource called VCF2GDS that load Variant Calling Format files into Genomic Data Storage objects that can be managed whiting R. This will be further illustrated when analyzing omic data analysis in this Section. Figure 7.1: Declaration of a resource corresponding to a VCF2GDS format 7.6 Examples with different resources Let us illustrate how to deal with different types of resources within DataSHIELD. To this end, let use our Opal test example available at https://opal-test.obiba.org which has the following reources Figure 7.2: Resources from a test enviroment available at https://opal-test.obiba.org Let us start by illustrating how to get a simple TSV file (brge.txt) into the R server. This file is located at a GitHub repository: https://raw.githubusercontent.com/isglobal-brge/brgedata/master/inst/extdata/brge.txt and it is not necesary to be moved from there. This is one of the main strenght of the resources implementation. 7.6.1 TSV file into a tibble or data.frame This code describes how to get the resource (a TSV file) as a data.frame into the R Server. Note that this is a secure access since user name and password must be provided library(DSI) library(DSOpal) library(dsBaseClient) # access to the &#39;brge&#39; resource (NOTE: test.brge is need since the project # is called test) builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-test.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;test.brge&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # the resource is loaded into R as the object &#39;res&#39; conns &lt;- DSI::datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # the resource is assigned to a data.frame # Assign to the original R class (e.g ExpressionSet) datashield.assign.expr(conns, symbol = &quot;dat&quot;, expr = quote(as.resource.data.frame(res))) ds.class(&quot;dat&quot;) $study1 [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; # logout the connection datashield.logout(conns) 7.6.2 .Rdata file into a specific R object Now let us describe how to get an specific type of R object into de R server. Our Opal test contains a resource called GSE80970 which is in a local machine. The resource is an R object of class ExpressionSet which is normally used to jointly capsulate gene expression, metadata and annotation. In general, we can retrieve any R object in their original format and if a method to coerce the specific object into a data.frame exists, we can also retrieve it as a tibble/data.frame. # prepare login data and resource to assign builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-test.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;test.GSE80970&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # login and assign resource (to &#39;res&#39; symbol) conns &lt;- DSI::datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # coerce ResourceClient objects to a data.frame called &#39;DF&#39; # NOTE: as.data.frame exists for `ExpressionSet` objects datashield.assign.expr(conns, symbol = &quot;DF&quot;, expr = quote(as.resource.data.frame(res))) ds.class(&quot;DF&quot;) $study1 [1] &quot;data.frame&quot; # we can also coerce ResourceClient objects to their original format. # This will allow the analyses with specific R/Bioconductor packages datashield.assign.expr(conns, symbol = &quot;ES&quot;, expr = quote(as.resource.object(res))) ds.class(&quot;ES&quot;) $study1 [1] &quot;ExpressionSet&quot; attr(,&quot;package&quot;) [1] &quot;Biobase&quot; # logout the connection datashield.logout(conns) "],
["setup.html", "8 Setup 8.1 Required DataSHIELD packages in the opal server 8.2 Required R Packages in the client site (e.g. local machine)", " 8 Setup As describe in a previous Chapter the resourcer R package allows to deal with the main data sources (using tidyverse, DBI, dplyr, sparklyr, MongoDB, AWS S3, SSH etc.) and is easily extensible to new ones including specific data infrastructure in R or Bioconductor. So far ExpressionSet and RangedSummarizedExperiment objects saved in .rdata files are accesible through the resourcer package. The dsOmics package contains a new extension that deals with VCF (Variant Calling Format) files which are coerced to a GDS (Genomic Data Storage) format (VCF2GDS). In order to achive this resourcer extension, two R6 classes have been implemented: GDSFileResourceResolver class which handles file-base resources with data in GDS or VCF formats. This class is responsible for creating a GDSFileResourceClient object instance from an assigned resource. GDSFileResourceClient class which is responsible for getting the referenced file and making a connection (created by GWASTools) to the GDS file (will also convert the VCF file to a GDS file on the fly, using SNPRelate). For the subsequent analysis, it’s this connection handle to the GDS file that will be used. 8.1 Required DataSHIELD packages in the opal server Required DataSHIELD packages must be uploaded in the opal server through the Administration site by accessing to DataSHIELD tab. In our case, both dsBase and dsOmics and resourcer packages must be installed as is illustrated in the figure. Figure 8.1: Installed packages in the test opal server The tab +Add package can be used to install a new package. The figure depicts how dsOmics was intalled into the opal server Figure 8.2: Description how dsOmics package was intalled into the test opal server 8.2 Required R Packages in the client site (e.g. local machine) In order to reproduce the results obtained in this part, the following R packages must be installed (in their development version): devtools::install_github(&quot;datashield/DSI&quot;, dependencies = TRUE) devtools::install_github(&quot;obiba/opalr&quot;, dependencies = TRUE) devtools::install_github(&quot;datashield/DSOpal&quot;, dependencies = TRUE) devtools::install_github(&quot;datashield/dsBaseClient&quot;, ref = &quot;v6.0-dev&quot;, dependencies = TRUE) devtools::install_github(&quot;isglobal-brge/dsOmicsClient&quot;, dependencies = TRUE) The package dependencies are then loaded as follows: library(DSI) library(DSOpal) library(dsBaseClient) library(dsOmicsClient) Notes: For advanced users willing to use DSLite, the server side packages needs to be installed as well: install.packages(&quot;resourcer&quot;, dependencies = TRUE) devtools::install_github(&quot;datashield/DSLite&quot;, dependencies = TRUE) devtools::install_github(&quot;isglobal-brge/dsOmics&quot;, dependencies = TRUE) devtools::install_github(&quot;datashield/dsBase&quot;, ref = &quot;v6.0-dev&quot;, dependencies = TRUE) "],
["statistical-analyses-from-different-resources.html", "9 Statistical analyses from different resources", " 9 Statistical analyses from different resources Let us start by illustrating how to peform simple statistical data analyses using different resources. Here, we will use data from three studies that are avaialbe in our OPAL test repository. The three databases are called CNSIM1, CNSIM2, CNSIM3 that are avaialble as three different resources: mySQL database, SPSS file and CSV file. This example mimics real situations were different hospital or research centers manage their own databases containing harmonized data. The analyses that are described here, can also be found in the DataSHIELD Tutorial where these resources where uploaded into the Opal server as three tables, a much worse approach since data have to be moved from original repositories. Let us start by illustrating how to analyze one data set. library(DSI) library(DSOpal) library(dsBaseClient) # prepare login data and resource to assign builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-test.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;test.CNSIM2&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # login and assign resource conns &lt;- DSI::datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # coerce ResourceClient objects to a data.frame called &#39;D&#39; datashield.assign.expr(conns, symbol = &quot;D&quot;, expr = quote(as.resource.data.frame(res))) Then we can inspect the type of data we have ds.class(&quot;D&quot;) $study1 [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; ds.colnames(&quot;D&quot;) $study1 [1] &quot;entity_id&quot; &quot;DIS_AMI&quot; &quot;DIS_CVA&quot; &quot;DIS_DIAB&quot; [5] &quot;GENDER&quot; &quot;LAB_GLUC_ADJUSTED&quot; &quot;LAB_HDL&quot; &quot;LAB_TRIG&quot; [9] &quot;LAB_TSC&quot; &quot;MEDI_LPD&quot; &quot;PM_BMI_CATEGORICAL&quot; &quot;PM_BMI_CONTINUOUS&quot; Perform some data descriptive analyses ds.table1D(&quot;D$DIS_DIAB&quot;) $counts D$DIS_DIAB 0 3041 1 47 Total 3088 $percentages D$DIS_DIAB 0 98.48 1 1.52 Total 100.00 $validity [1] &quot;All tables are valid!&quot; ds.table2D(&quot;D$DIS_DIAB&quot;, &quot;D$GENDER&quot;) $colPercent $colPercent$`study1-D$DIS_DIAB(row)|D$GENDER(col)` 0 1 Total 0 98.04 98.94 98.48 1 1.96 1.06 1.52 Total 100.00 100.00 100.00 $colPercent.all.studies $colPercent.all.studies$`pooled-D$DIS_DIAB(row)|D$GENDER(col)` 0 1 Total 0 98.04 98.94 98.48 1 1.96 1.06 1.52 Total 100.00 100.00 100.00 $rowPercent $rowPercent$`study1-D$DIS_DIAB(row)|D$GENDER(col)` 0 1 Total 0 51.10 48.90 100 1 65.96 34.04 100 Total 51.33 48.67 100 $rowPercent.all.studies $rowPercent.all.studies$`pooled-D$DIS_DIAB(row)|D$GENDER(col)` 0 1 Total 0 51.10 48.90 100 1 65.96 34.04 100 Total 51.33 48.67 100 $chi2Test $chi2Test$`study1-D$DIS_DIAB(row)|D$GENDER(col)` Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: contingencyTable X-squared = 3.5158, df = 1, p-value = 0.06079 $chi2Test.all.studies $chi2Test.all.studies$`pooled-D$DIS_DIAB(row)|D$GENDER(col)` Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: pooledContingencyTable X-squared = 3.5158, df = 1, p-value = 0.06079 $counts $counts$`study1-D$DIS_DIAB(row)|D$GENDER(col)` 0 1 Total 0 1554 1487 3041 1 31 16 47 Total 1585 1503 3088 $counts.all.studies $counts.all.studies$`pooled-D$DIS_DIAB(row)|D$GENDER(col)` 0 1 Total 0 1554 1487 3041 1 31 16 47 Total 1585 1503 3088 $validity [1] &quot;All tables are valid!&quot; Or even some statistical modelling. In this case we want to assess whether sex (GENDER) or trigrycerids (LAB_TRIG) are risk factors for diabetes (DIS_DIAB) mod &lt;- ds.glm(DIS_DIAB ~ LAB_TRIG + GENDER, data = &quot;D&quot; , family=&quot;binomial&quot;) mod$coeff Estimate Std. Error z-value p-value low0.95CI.LP high0.95CI.LP P_OR (Intercept) -5.0254035 0.39487495 -12.726570 4.209299e-37 -5.7993442 -4.2514628 0.006526066 LAB_TRIG 0.3462886 0.09880155 3.504891 4.567950e-04 0.1526411 0.5399361 1.413810621 GENDER -0.4550068 0.39497696 -1.151983 2.493280e-01 -1.2291474 0.3191338 0.634443636 low0.95CI.P_OR high0.95CI.P_OR (Intercept) 0.00302039 0.01404336 LAB_TRIG 1.16490687 1.71589723 GENDER 0.29254188 1.37593539 As usual the connection must be closed datashield.logout(conns) "],
["Omic.html", "10 Using the resources for transcriptomic and epigenomic data analysis with Bioconductor", " 10 Using the resources for transcriptomic and epigenomic data analysis with Bioconductor "],
["extending-the-resources-to-deal-with-vcf-file-in-genomic-data-analysis-gwas-with-bioconductor.html", "11 Extending the resources to deal with VCF file in genomic data analysis: GWAS with Bioconductor", " 11 Extending the resources to deal with VCF file in genomic data analysis: GWAS with Bioconductor "],
["omic-extension.html", "12 Extending the resources to SHELL programs: GWAS with PLINK", " 12 Extending the resources to SHELL programs: GWAS with PLINK "],
["GIS.html", "13 Extending the resources to Geographical Information System (GIS) analyses", " 13 Extending the resources to Geographical Information System (GIS) analyses "],
["dslite-datashield-implementation-on-local-datasets.html", "14 DSLite: DataSHIELD Implementation on Local Datasets 14.1 Introduction 14.2 Development Environment Setup 14.3 DataSHIELD Development Flow", " 14 DSLite: DataSHIELD Implementation on Local Datasets 14.1 Introduction DSLite is a serverless DataSHIELD Interface (DSI) implementation which purpose is to mimic the behavior of a distant (virtualized or barebone) data repository server (see DSOpal for instance). The datasets that are being analyzed must be fully accessible in the local environment and then the non-disclosive constraint of the analysis is not relevant for DSLite: some DSLite functionalities allows to inspect what is under the hood of the DataSHIELD computation nodes, making it a perfect tool for DataSHIELD analysis package developers. 14.2 Development Environment Setup 14.2.1 DataSHIELD Packages Both client and server side packages must be installed in your local R session. The entry point is still the client side package and DSLite will automatically load the corresponding server side package on DataSHIELD aggregate and assignment functions call, based on the DataSHIELD configuration. 14.2.2 Test Datasets DSLite comes with a set of datasets that can be easily loaded. You can also provide your own to illustrate a specific data analysis function. 14.2.3 R Package Development Tools We recommend using the following tools to facilitate R package development: devtools, the collection of package development tools, usethis, automate package and project setup tasks that are otherwise performed manually, testthat, for unit testing, roxygen2, for writing documentation in-line with code, Rstudio, the R editor that integrates the tools mentioned above and more. 14.3 DataSHIELD Development Flow The typical development flow, using DSLite, is: Build and install your client and/or server side DataSHIELD packages. Create a new DSLiteServer object instance, refering test datasets. Use or alter the default DataSHIELD configuration. Test your DataSHIELD client/server functions. Debug DataSHIELD server nodes using DSLiteServer methods. 14.3.1 DSLiteServer After your client and/or server side DataSHIELD packages have been built and installed, a new DSLiteServer object instance must be created. Some DSLiteServer methods can be used to verify or modify the DSLiteServer behaviour: DSLiteServer$strict() DSLiteServer$home() See the R documentation of the DSLiteServer class for details. As an example: library(DSLite) # prepare test data in a light DS server data(&quot;CNSIM1&quot;) data(&quot;CNSIM2&quot;) data(&quot;CNSIM3&quot;) dslite.server &lt;- newDSLiteServer(tables=list(CNSIM1=CNSIM1, CNSIM2=CNSIM2, CNSIM3=CNSIM3)) # load corresponding DataSHIELD login data data(&quot;logindata.dslite.cnsim&quot;) The previous example can be simplified using the set-up functions based on the provided test datasets: library(DSLite) # load CNSIM test data logindata.dslite.cnsim &lt;- setupCNSIMTest() 14.3.2 DataSHIELD Configuration The DataSHIELD configuration (aggregate and assign functions, R options) is automatically discovered by inspecting the R packages installed and having some DataSHIELD settings defined, either in their DESCRIPTION file or in a DATASHIELD file. This default configuration extracting function is: DSLite::defaultDSConfiguration() The list of the DataSHIELD R packages to be inspected (or excluded) when building the default configuration can be specified as parameters of defaultDSConfiguration(). The DataSHIELD configuration can be specified at DSLiteServer creation time or afterwards with some DSLiteServer methods that can be used to verify or modify the DSLiteServer configuration: DSLiteServer$config() DSLiteServer$aggregateMethods() DSLiteServer$aggregateMethod() DSLiteServer$assignMethods() DSLiteServer$assignMethod() DSLiteServer$options() DSLiteServer$option() See the R documentation of the DSLiteServer class for details. As an example: # verify configuration dslite.server$config() 14.3.3 DataSHIELD Sessions The following figure illustrates a setup where a single DSLiteServer holds several data frames and is used by two different DataSHIELD Connection (DSConnection) objects. All these objects live in the same R environment (usually the Global Environment). The “server” is responsible for managing DataSHIELD sessions that are implemented as distinct R environments inside of which R symbols are assigned and R functions are evaluated. Using the R environment paradigm ensures that the different DataSHIELD execution context (client and servers) are contained and exclusive from each other. DSLite architecture After performing the login DataSHIELD phase, the DSLiteServer holds the different DataSHIELD server side sessions, i.e. R environments identified by an ID. These IDs are also stored within the DataSHIELD connection objects that are the result of the datashield.login() call. The folllowing example shows how to access these session IDs: # datashield logins and assignments conns &lt;- datashield.login(logindata.dslite.cnsim, assign=TRUE) # get the session ID of &quot;sim1&quot; node connection object conns$sim1@sid # the same ID is in the DSLiteServer dslite.server$hasSession(conns$sim1@sid) 14.3.4 Debugging Thanks to the DSLiteServer capability to have its configuration modified at any time, it is possible to add some debugging functions without polluting in the DataSHIELD package you are developping. For instance, this code adds an aggregate function print(): # add a print method to configuration dslite.server$aggregateMethod(&quot;print&quot;, function(x){ print(x) }) # and use it to print the D symbol datashield.aggregate(conns, quote(print(D))) Another option is to get a symbol value from the server into the client environment. This can be very helpful for complex data structures. The following example illustrates usage of a shortcut function that iterates over all the connection objects and get the corresponding symbol value: # get data represented by symbol D for each DataSHIELD connection data &lt;- getDSLiteData(conns, &quot;D&quot;) # get data represented by symbol D from a specific DataSHIELD connection data1 &lt;- getDSLiteData(conns$sim1, &quot;D&quot;) 14.3.5 Limitations 14.3.5.1 Function Parameters Parser The main difference with a regular DSI implementation (such as the one of DSOpal) is that the arguments of the DataSHIELD functional calls are not parsed in DSLite. The only R language element that is inspected and handled is the name of the functions, that are replaced by the ones defined in the DataSHIELD configuration. For instance the following expression, which includes a function call in the formula, is valid for the DSLiteServer but not for Opal: someregression(D$height ~ D$diameter + poly(D$length,3,raw=TRUE)) As a consequence, DataSHIELD R package development can take advantage of DSLite flexibility for speeding development but will never replace testing on a regular DataSHIELD infrastructure using DSOpal. 14.3.5.2 Server Side Environments For each of the DataSHIELD node, the server side code is evaluated within an environment that has no parent, i.e. detached from the global environment where the client code is executed. Some R functions have a parameter that allows to specify to which environment they apply, for instance assign(), get(), eval(), as.formula(), etc. Their env (or envir) parameter default value is parent.frame() which is the global environment when executed in Opal’s R server, because it is the parent frame of the package’s namespace where the function is defined. In DSLiteServer, the parent frame must be the environment where the server code is evaluated. In order to be consistent between these two execution contexts (Opal R server and DSLiteServer), you must specify the env (or envir) value explicitly to be parent.frame(), which is the parent frame of the block being executed (either the global environment in Opal context, or the environment defined in DSLiteServer). Example of a valid server side piece of code that assigns a value to a symbol in the DataSHIELD server’s environment (being the Opal R server’s global environment or a DSLiteServer’s environment): base::assign(x = &quot;D&quot;, value = someValue, envir = parent.frame()) See also the Advanced R - Environments documentation to learn more about environments. "],
["creating-datashield-packages.html", "15 Creating DataSHIELD packages", " 15 Creating DataSHIELD packages "],
["useful-information.html", "16 Useful information 16.1 How to install R packages into OPAL server from R 16.2 How to check whether there are open R sesions in the OPAL server 16.3 How", " 16 Useful information 16.1 How to install R packages into OPAL server from R 16.2 How to check whether there are open R sesions in the OPAL server 16.3 How "]
]
